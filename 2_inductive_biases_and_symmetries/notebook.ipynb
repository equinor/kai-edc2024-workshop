{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient training of neural networks\n",
    "\n",
    "In the previous notebook, we outlined how neural networks can solve a large variety of tasks. Specifically, we demonstrated that neural networks can be used for image classification and for predicting complex biological processes, such as the conductance of nerve membranes. The main reason for this verstility of neural networks is two-fold: \n",
    "1. neural networks are universal function approximators, meaning they can learn any function from data; and \n",
    "2. surprisingly many tasks can be approximated as input-output relationships, i.e. functions. \n",
    "\n",
    "For example, we saw that digit classification can be achieved by learning a probability density function, $p(y | x)$, where $x$ denotes the pixels in the image, and $y$ denotes the digit. In this notebook, we dive deeper into some of the challenges of learning functions from data -- challenges that are important to understand when modelling language with neural networks.\n",
    "\n",
    "**Learning outcomes:**\n",
    "\n",
    "1. An intuitive understanding of architectural choices for neural networks\n",
    "2. An understanding of the limitations of neural networks when learning functions from data\n",
    "3. The value of prior knowledge for more efficient learning and better generalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "import kai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Using neural networks for language processing\n",
    "\n",
    "The universal function approximation theorem states that multi-layer perceptrons (MLPs) can approximate any function, provided they have sufficiently many artificial neurons. This makes it possible to learn any function from data using neural networks. At this point, you might start to wonder what all of this has to do with natural language processing and large language models... After all, language seems to be quite different from simple input-output relationships, making it hard to imagine how the universality of neural networks helps with processing language.\n",
    "\n",
    "But how different is language from an input-output relationship really? If we think, for example, about chatbots like our very own Equinor AIChat, typical interactions with the chatbot often take the form of questions (inputs) and answers (outputs): We ask a question or make a statement; and the chatpot replies. Hmm, so perhaps some elements of language can be interpreted as an input-output relationship after all!\n",
    "\n",
    "That's exciting! Let us try to develop a neural network that can answer questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Brainstorm how you could use an MLP to answer questions\n",
    "\n",
    "Assume for now that we have a fixed number of questions that can be asked and that also the number of answers to each question is fixed. What could the inputs to the network represent? What would the outputs be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can document your thoughts here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: What are the limitations of the Q&A MLP?\n",
    "\n",
    "After making an initial attempt to define suitable inputs and outputs to the Q&A neural network, have a look at some example questions and answers below. Do you foresee any challenges with answering those questions with your approach? Think especially about how well your model might answer questions that it has not seen before, or even how it might answer questions that only use a slightly different wording than those in the datatset. \n",
    "\n",
    "*Tipp:* If you struggle to see the limitations of your Q&A neural network, let us simply try it out and see how well it does! Create a code cell and use the Q&A data to train an MLP. You can copy the neural network definition and the training script from the previous notebook. For simplicity, you could just focus on the first 50 questions and answers in the dataset and use the ``nn.CrossEntropyLoss()`` as an objective for the training. Remember to split your dataset into a training and into a test dataset to test your model on unseen questions. How well can your model answer the questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = kai.get_qa_datatset()\n",
    "\n",
    "print(data[0])\n",
    "print('')\n",
    "print(data[30])\n",
    "print('')\n",
    "print(data[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, it's obvious that the Equinor AIChat does not use our simple Q&A neural network approach! While it is possible to enumerate questions and answers and to train a neural network to remember the mapping between questions and answers, it does not actually allow the neural network to answer any questions that are not identical to the training questions. In fact, by enumerating questions and answers, we abstract away any logic or knowledge that a human might be able to infer from the question-answer pairs, making it impossible to answer any new questions, even if the answers are contained in the training data.\n",
    "\n",
    "For example, if the training data contained the question-answer pairs:\n",
    "\n",
    "- Q: How long have Alice and Bob been living together? A: Alice and Bob have been living together for 10 years.\n",
    "- Q: Who is Bob's neighbour? A: Bob's neighbour is Anders Opedal.\n",
    "\n",
    "A human can infer that Alice's neighbour is Ander Opedal. But if we asked our simple Q&A neural network \"Who is Alice's neighbour?\", it would not be able to answer the question. In contrast, modern large language models *can* make similar inferences and answer many questions that they have not seen before. So we seem to be missing an important component of developing neural networks to make natural language processing work.\n",
    "\n",
    "Note that, in principle, our Q&A neural network *can* answer any questions provided they are contained in the training data. It simply cannot make any inferences or generalise beyond the training questions in its current formulation. I.e. the universality of neural network approximations is not enough to solve natural language processing in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Universal, but not generalisable\n",
    "\n",
    "The challenges of using MLPs for learning function approximations that generalise beyond the training data are not unique to language processing. In fact, one of the first approaches that improved the generalisability of neural networks came from image classification. This advancement ultimately led to the development of the LLM neural networks that we are using today. We will therefore focus in this notebook on understanding this approach.\n",
    "\n",
    "To begin, let us revisit the MNIST image classification task that we have worked on in the previous notebook. Below we have defined the MLP for the image classification. The only new element to the previous notebook is that we have slighly increased the size of the images by padding the space around the digits (see cell below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_loader, test_loader1, test_loader2 = kai.get_modified_mnist_data()\n",
    "\n",
    "# Get a batch of images and labels\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "kai.plot_images(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Train the neural network on the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Set up training\n",
    "input_size = 784 * 4  # Need to increase input size because images are bigger\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize Network\n",
    "model = NeuralNetwork(input_size=input_size, num_classes=num_classes).to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train Network\n",
    "kai.train_mnist_model(\n",
    "    num_epochs, model, objective, optimizer, train_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Check that the accuracy on the training data is still the same\n",
    "You should get an accuracy of >95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy on training data:')\n",
    "kai.check_accuracy(train_loader, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing! The padding of the images had no effect on the training accuracy -- and why would it?!? The digits in the images remain the same, whether there is black space around them or not. So let's verify that the accuracy also is still the same on the test images (which we haven't used for the training)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Evaluate the accuracy of the model on the two test datasets\n",
    "The test datasets are defined by ``test_loader1`` and by ``test_loader2``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check the accuracy of the model on the test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! The accuracy for one of the datasets is significantly worse than for the other. \n",
    "\n",
    "### Task 6: Discuss the reasons for why the model may perform differently on different test datasets\n",
    "\n",
    "Perhaps, visualising a few images from the test datasets can help you identify the reason..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot a few images from the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doument your thoughts here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Inductive biases\n",
    "\n",
    "One of the first approaches that tackled this lack of \"concept learning\" was developed by Yann Lecun, who is now a Computer Science professor at NYU and the Chief AI scientist at Meta. In his seminal paper, Lecun proposed to exploit prior knowledge about the image classification task to make the neural network more efficient at learning from data and to help it generalise more easily beyond the training images (see https://hal.science/hal-03926082/document). More specifically, Lecun argued that image classification should not be sensitive to translations of the object of interest (i.e. the location of the digit in the image, see figure below). He therefore proposed the use of neural networks that are explicitly designed to make predictions which are invariant under these translations.\n",
    "\n",
    "<img src=\"translation_invariance.gif\" width=\"500\" style=\"display: block; margin: 0 auto\" >\n",
    "\n",
    "His proposal to achieve this was to slighty modify how MLPs process images by introducing small blocks of artifical neurons that are slided across all patches of the image (see figure below). In this way, patterns learned in one region of the image can also be recognised in other regions of the image. These neural network layers are known as convolutional layers (in reference to functional analysis and physics, see https://en.wikipedia.org/wiki/Convolution).\n",
    "\n",
    "<img src=\"convolution.gif\" width=\"500\" style=\"display: block; margin: 0 auto\" >\n",
    "\n",
    "Using convolutional layers, Lecun defined the first convolutional neural network (CNN) which was the first neural network that was able to learn something close to a concept of a digit in order to classify images correctly beyond the training dataset (see below). His network consisted of 2 convolutional layers, each followed by a pooling layer. We will work through these details below. Note that the final component of the network remains a MLP (see the final two \"dense\" layers in the figure). Dense layers are the layers that we have used before (``nn.Linear``), where each input is processed by all artificial neurons. Dense layers are also sometimes referred to as fully connected layers. \n",
    "\n",
    "<img src=\"Lenet.svg\" width=\"500\" style=\"display: block; margin: 0 auto\" >\n",
    "\n",
    "Below we will implement Lecun's CNN, but before we do so, we just want to comment on the quite substantially more complex-looking architecture of the neural network in comparison to our \"simple\" MLPs that we used in the previous notebook. While Lecun's CNN looks complex at first, conceptually, its architecture is actually quite intuitive: it has one block that is responsible for feature extraction (the convolutional layers); and another block that is responsible for the classification (the dense layers). The feature extractor is based on prior knowledge of the image classification task and it implements inductive biases that help the neural network to learn more efficiently from data and to generalise more easily beyond the training data. The dense layers are not based on any prior assumptions about the modelling task and they simply make use of the universality of MLPs to map the extracted features to the probability masses of the digits. This combination of the universal function approximation theorem and inductive biases is one of the main reasons for the wide success of neural networks in recent years (including language processing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Implement a convolutional layer\n",
    "\n",
    "A convolutional layer processes inputs similar to the dense layers (``nn.Linear``) that we have used before, the only difference is that now a smaller number of artificial neurons is slided across the inputs, such that patterns learned from one patch of signals can also benefit the processing of another patch of signals. The neurons process the inputs as before by: 1. mutliplying the signals by model weights; 2. summing the weighted signals; 3. adding a bias; and 4. applying an activation function.\n",
    "\n",
    "For images, this \"patch processing\" takes the form of sliding a small window of model weights across the image (see below). After multiplying the inputs by the weights, the values are summed up and a bias is added to produce the value for one patch of signals before applying the activation function. In the figure below, the bias is equal to 0.\n",
    "\n",
    "<img src=\"convolutional_layer.gif\" width=\"500\" style=\"display: block; margin: 0 auto\" >\n",
    "\n",
    "Implement a convolutional layer that takes an image (two-dimensional array of shape (n, m)), the model weights (two-dimensional array of shape (k, k)) and the bias (float) as input, and it returns the processed image. For simplicity, we will choose a window of size 3x3 and assume that the input image is larger than 3x3 (just like in the image above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the implementation\n",
    "def convolutional_layer(inputs, weights, bias):\n",
    "    inputs = np.array(inputs)\n",
    "    m, n = inputs.shape\n",
    "    if (m <= 3) or (n <= 3):\n",
    "        raise ValueError('Inputs has the wrong shape.')\n",
    "    weights = np.array(weights)\n",
    "    k1, k2 = weights.shape\n",
    "    if (k1 != 3) or (k2 != 3):\n",
    "        raise ValueError('Weights has the wrong shape.')\n",
    "    bias = float(bias)\n",
    "\n",
    "    # TODO Implement the convolution\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "# Test 1\n",
    "inputs = np.array([\n",
    "    [1, 1, 1, 0, 0],\n",
    "    [0, 1, 1, 1, 0],\n",
    "    [0, 0, 1, 1, 1],\n",
    "    [0, 0, 1, 1, 0],\n",
    "    [0, 1, 1, 0, 0]\n",
    "])\n",
    "weights = np.array([\n",
    "    [1, 0, 1],\n",
    "    [0, 1, 0],\n",
    "    [1, 0, 1]\n",
    "])\n",
    "bias = 0\n",
    "output = convolutional_layer(inputs, weights, bias)\n",
    "true = np.array([\n",
    "    [4, 3, 4],\n",
    "    [2, 4, 3],\n",
    "    [2, 3, 4]\n",
    "])\n",
    "np.testing.assert_equal(output, true)\n",
    "\n",
    "# Test 2\n",
    "bias = 5\n",
    "output = convolutional_layer(inputs, weights, bias)\n",
    "true = np.array([\n",
    "    [9, 8, 9],\n",
    "    [7, 9, 8],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "np.testing.assert_equal(output, true)\n",
    "\n",
    "# Test 3\n",
    "weights = np.array([\n",
    "    [1, 0, 1],\n",
    "    [0, 0, 0],\n",
    "    [1, 0, 1]\n",
    "])\n",
    "output = convolutional_layer(inputs, weights, bias)\n",
    "true = np.array([\n",
    "    [8, 7, 8],\n",
    "    [7, 8, 7],\n",
    "    [7, 7, 8]\n",
    "])\n",
    "np.testing.assert_equal(output, true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8: Discuss the purpose of a pooling layer\n",
    "\n",
    "In Lecun's CNN each convolutional layer is followed by a pooling layer. A pooling layer aggregates neighbouring inputs into one output. This aggregation can take different forms. For example, the figure below shows a \"max pooling\" on the left and an \"average pooling\" on the right. For the max pooling, the output is the maximum value among the cluster of neighbouring pixels, while for the average pooling, the average value across the cluster of neighbouring pixels is returned.\n",
    "\n",
    "<img src=\"average_pooling.gif\" width=\"500\" style=\"display: block; margin: 0 auto\" >\n",
    "\n",
    "Discuss in your group the purpose of pooling layers for the image classification task. Do they make features extracted by convolutional layers more robust? Are they simply a way of condensing the amount of information that is being processed? Do they have another purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can document your thoughts here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9: Implement Lecun's CNN (LeNet)\n",
    "\n",
    "Below is a diagram of Lecun's CNN, referred to as LeNet. Use PyTorch's pre-implemented layers (``nn.Conv2d``, ``nn.AvgPool2d``, ``nn.Linear``) to implement the LeNet neural network. You can find the API definition of the layers here: https://pytorch.org/docs/stable/nn.html. When you are finished implementing the neural network, train it on the MNIST dataset. We will first train it on the original MNIST dataset to establish that the network works as expected.\n",
    "\n",
    "<img src=\"lenet-architecture.svg\" width=\"150\" style=\"display: block; margin: 0 auto\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the implementation\n",
    "class LeNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # TODO: Define the layers of the network\n",
    "\n",
    "    def forward(self,x):\n",
    "        # TODO: Chain the layers to process images according to the LeNet architecture\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that the implementation of the network is correct\n",
    "model = LeNet()\n",
    "s = summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "batch_size = 64\n",
    "\n",
    "# Load Data\n",
    "train_dataset = datasets.MNIST(root='dataset/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='dataset/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Network\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LeNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "kai.train_mnist_model(\n",
    "    num_epochs, model, objective, optimizer, train_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the performance of the model\n",
    "kai.check_accuracy(train_loader, model, device)\n",
    "kai.check_accuracy(test_loader, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing! You have just implemented your first CNN. Let us keep the momentum high and train the CNN on the modified MNIST dataset to see whether it is better at generalising beyond the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10: Train the model on the modified MNIST dataset\n",
    "Note that you have to slightly modify the neural network, because increasing the size of the input images will change the size of the feature maps, and the first fully connected layer of the classifier assumes an input size of 16 x 5 x 5 = 400. This is because the outputs of the final convolutional block (convolutional layer and pooling layer) are 16 5x5 feature images. How does the size of the feature images change when the input image size is 4 times larger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update the model, so it can process the larger images\n",
    "class LeNet2(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet2, self).__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            # TODO: Change the input size of the linear layer\n",
    "            nn.Linear(400, 120),\n",
    "\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, 10),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.feature_extractor(x)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the implementation of the modified model\n",
    "model = LeNet2()\n",
    "s = summary(model, input_size=(1, 28*2, 28*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modified MNIST dataset (kai.get_modified_mnist_data)\n",
    "train_loader, test_loader1, test_loader2 = kai.get_modified_mnist_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Network\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LeNet2().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "kai.train_mnist_model(\n",
    "    num_epochs, model, objective, optimizer, train_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the performance of the model\n",
    "kai.check_accuracy(train_loader, model, device)\n",
    "kai.check_accuracy(test_loader1, model, device)\n",
    "kai.check_accuracy(test_loader2, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, that was a lot of work for no improvement on the second test dataset at all... But don't worry, we are almost there! Why do you think does the LeNet network not improve on the test dataset and what can we do to fix it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 11: Discuss in your group why the LeNet network does not achieve translational invariance afterall\n",
    "\n",
    "*Tip:* Perhaps, visualising the outputs from the feature extractor for some test images can help you see the problem. You can use the ``kai.plot_images`` function if your first convert the torch tensors to numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot a few images from the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can document your thoughts here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 12: Modify the final pooling layer in the LeNet neural network\n",
    "\n",
    "The output of the pooling layer should be just one pixel for each feature channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet3(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet3, self).__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            # TODO: Change the pooling to return just one value per feature map\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            # TODO: Also match the number of inputs here\n",
    "            nn.Linear(12*12*16, 120),\n",
    "\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, 10),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.feature_extractor(x)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the implementation of the modified model\n",
    "model = LeNet3()\n",
    "s = summary(model, input_size=(1, 28*2, 28*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Network\n",
    "learning_rate = 0.005\n",
    "num_epochs = 10\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LeNet3().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "kai.train_mnist_model(\n",
    "    num_epochs, model, objective, optimizer, train_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the performance of the model\n",
    "kai.check_accuracy(train_loader, model, device)\n",
    "kai.check_accuracy(test_loader1, model, device)\n",
    "kai.check_accuracy(test_loader2, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Magic! With this simple tweak we managed to enable the neural network to learn generalisable concepts ðŸŽ‰\n",
    "\n",
    "Feel free to play around with the network to get it's performance on the training dataset back up to >95%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

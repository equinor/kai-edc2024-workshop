{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Artificial Intelligence?\n",
    "\n",
    "In this notebook, we provide an intuitive introduction to the foundations of artificial intelligence.\n",
    "\n",
    "**Learning outcomes:**\n",
    "- how biological brains inspired the development of artificial neural networks in the hope of emergent intelligent behaviour in artificial brains;\n",
    "- how multi-layer perceptrons (MLPs) provide the backbone of all modern neural networks;\n",
    "- and how MLPs can approximate any function (universal function approximation theorem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pints.toy import FitzhughNagumoModel\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import kai\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 The Perceptron\n",
    "\n",
    "Intuitively, one might expect artificial intelligence (AI) to arise when detailed models of the human brain are simulated on a computer. This intuition motivated the development of artificial neural networks more than 60 years ago. These artificial neural networks are now at the core of all modern AI models. In this notebook, we will retrace these early developments of artificial intelligence to better understand why language models, such as GPT3, process language the way they do.\n",
    "\n",
    "The smallest building blocks of brains are brain cells, also known as neurons. Each neuron by itself is not more intelligent than any other cell in our body. But our brains are composed of approximately 100 billion neurons which communicate with each other and process information in a coordinated way. This is widely believed to lead to the emergence of intelligence. A good start for artificial intelligence is therefore to first build a model of a neuron, and then make many of those neurons communicate in the hope to see artificial intelligence emerge in a similar way to biological neural networks.\n",
    "\n",
    "In the figure below, we illustrate a human neuron in the left panel. The right panel shows a model of the neuron: an artificial neuron, commonly referred to as *perceptron*. The biological neuron consists of a junction of dendrites with the cell nucleus in the middle, and an axon leading away from the neuron. Neurons receive electrical signals through their dendrites and forward strong enough signals to other neurons along their axons. The perceptron emulates this behaviour by receiving $n$ input signals $x_1, \\ldots, x_n$ which are processed into one output signal $y$.\n",
    "\n",
    "<img src=\"biological-neuron.jpg\" width=\"500\" style=\"display: block; margin: 0 auto\" >\n",
    "\n",
    "More specifically, the perceptron in the figure is defined by a linear combination of the input signals and a step function that only returns a value of $1$ when the weighted sum is greater than zero \n",
    "\n",
    "$$\n",
    "y = f(x, w) = \\Theta (w_0\\, x_0 + \\ldots + w_n\\, x_n),\n",
    "$$\n",
    "where $\\Theta(z) = 1$ for $z>0$, and $\\Theta(y) = 0$ otherwise. This activation function, $\\Theta(z)$, mimics the behaviour of neurons to only forward signals if the incoming signals are large enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement a simple artificial neuron\n",
    "\n",
    "To familiarise yourselfves with how perceptrons process information, implement a perceptron by completing the code block below. When you are ready, use the notebook cell after that to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(x):\n",
    "    \"\"\"\n",
    "    Returns the output of a perceptron with 6 input signals.\n",
    "\n",
    "    Parameters:\n",
    "        x (List of length 6) Input signals to the perceptron.\n",
    "    \"\"\"\n",
    "    weights = [0.2, -0.3, 0.9, 0.4, -0.9, -0.5]\n",
    "\n",
    "    # TODO: Calculate y\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the below cell to chek your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "kai.plot_perceptron(perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Example\n",
    "\n",
    "How can we understand perceptron's processing information in a less abstract way? To this end, let us consider an example, and more specifically, let's say we want to answer the question: \"Will Liza go to Emma's party?\".\n",
    "\n",
    "There are certain factors that influence Liza's decision. For instance, if her friend Hannah is going to the party, Liza would not like to go. However, if Kristine is going, Liza would be delighted to attend. If neither of them are going, Liza would like to stay at home.\n",
    "\n",
    "We can represent this situation using the given setup where the values $w_{1}$ and $w_{2}$ represent the Hannah's and Kristine's influence on Liza's decision, respectively. Negative values are used to denote negative influence (i.e., making Liza less likely to attend the party), while positive values indicate positive impact.\n",
    "\n",
    "<img src=\"emmas_party.png\" width=\"500\" style=\"display: block; margin: 0 auto\">\n",
    "\n",
    "When both Hannah and Kristine are going, we can calculate the activation of the model. By using the values $w_{1}= -3$,$w_{1}= 2$ and $w_{1}= -1$, the model outputs an activation of 0. This means that according to the model, Liza will not go to the party.\n",
    "$$\n",
    "z = x_{1}w_{1} + x_{2}w_{2} + b = 1(-3) + 1(2) - 1 = -2\n",
    "a = f(z) = 0\n",
    "$$\n",
    "\n",
    "This example demonstrates how the weights and biases can be tuned to make decisions based on inputs. The right combination of weights and biases can help the model make accurate predictions based on the given inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 From single perceptrons to networks of perceptrons\n",
    "\n",
    "Single perceptrons are mathematically equivalent to linear regression models, if we omit the activation function (in the above example this was the step function, $\\Theta(z)$)\n",
    "$$\n",
    "    z = \\sum _{j=1}^n w_j x_j.\n",
    "$$\n",
    "Linear regression models have been used in the machine learning community for a long time and they can help us interpret how perceptrons process information. For example,for just two inputs, we can visualise the output of a perceptron using a straight line in a plane, see below. Note that it is convential to consider $x_0$ to be fixed to one, $x_0=1$. This enables translations of the lines in the plane. The corresponding weight, $w_0$, is commonly referred to as the bias of the perceptron (represented by the constant value in the figure). \n",
    "\n",
    "<img src=\"linear_classifier.png\" width=\"500\" style=\"display: block; margin: 0 auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation of perceptrons as binary linear regressors, i.e. linear classifiers, makes intuitively clear how the smallest building blocks of artificial neural networks process information. But one perceptron alone is limited in what it can do and certainly does not achieve artificial \"intelligence\".\n",
    "\n",
    "To visualise the limitations of a single perceptron, let us attempt a simple 2-dimensional classification task using the perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random data\n",
    "np.random.seed(0)\n",
    "n_samples = 50\n",
    "X = np.random.randn(n_samples, 2)\n",
    "y = np.zeros(n_samples)\n",
    "y[np.linalg.norm(X, axis=1) > 1] = 1\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], label='Class 1')\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], label='Class 2')\n",
    "\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-6, 6)\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the task for the perceptron is to classify the points based on the $x_1$ and $x_2$ coordinates. It is clear that we will not be able to use just a single line to separate the blue dots from the orange dots. But, we could, perhaps, use 4 perceptrons to classify the data..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Find 4 perceptrons that can jointly classify the data points\n",
    "For example, you could decide that a data point is classified as \"blue\" if all perceptrons return a 1. Otherwise the data point is classified as orange. You can use the below code block to define the weights of the perceptrons, and the code block after that to visualise their decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the dictionaries to define weights and biases of the perceptrons\n",
    "perceptron1 = {\n",
    "    'bias': -2,\n",
    "    'weight1': 2,\n",
    "    'weight2': 1\n",
    "}\n",
    "perceptron2 = {\n",
    "    'bias': ,      # TODO\n",
    "    'weight1': ,   # TODO\n",
    "    'weight2':     # TODO\n",
    "}\n",
    "perceptron3 = {\n",
    "    'bias': ,      # TODO\n",
    "    'weight1': ,   # TODO\n",
    "    'weight2':     # TODO\n",
    "}\n",
    "perceptron4 = {\n",
    "    'bias': ,      # TODO\n",
    "    'weight1': ,   # TODO\n",
    "    'weight2':     # TODO\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1])\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1])\n",
    "\n",
    "# linear classifications\n",
    "x1 = np.linspace(-3, 3, 100)\n",
    "x2 = -(perceptron1['weight1'] * x1 + perceptron1['bias']) / perceptron1['weight2']\n",
    "plt.plot(x1, x2, linestyle='-', color='red')\n",
    "\n",
    "x2 = -(perceptron2['weight1'] * x1 + perceptron2['bias']) / perceptron2['weight2']\n",
    "plt.plot(x1, x2, linestyle='-', color='orange')\n",
    "\n",
    "x2 = -perceptron3['weight1'] / perceptron3['weight2'] * x1 - perceptron3['bias'] / perceptron3['weight2']\n",
    "plt.plot(x1, x2, linestyle='-', color='lightblue')\n",
    "\n",
    "x2 = -perceptron4['weight1'] / perceptron4['weight2'] * x1 - perceptron4['bias'] / perceptron4['weight2']\n",
    "plt.plot(x1, x2, linestyle='-', color='darkcyan')\n",
    "\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-6, 6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise demonstrates that networks of artificial neurons can perform more complex tasks than single perceptrons. The figure below illustrates this network diagrammatically.\n",
    "\n",
    "<img src=\"complex_classifier_perceptron_diagram.png\" width=\"1000\" style=\"display: block; margin: 0 auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, so far our neural network does not feel very \"intelligent\". It can only classify points in a plane, and we had to painfully tweak the weights of the network to do it. More complex networks are able to perform more complex tasks (modern neural networks, such as large language models, have up to 200 billion model weights), but we need to find a way to avoid manually tweaking the weights of network networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Training neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The automatic tweaking of neural network weights is achieved using numerical optimisation algorithms and is referred to as \"training\" or as \"learning\" in the AI community. While \"training\" and \"learning\" sounds fancy, the high level concept of automatically tweaking network weights to optimise an objective is quite intuitive..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Discuss in your group how you would design an iterative 3-step algorithm that can optimise neural network weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the below algorithm:\n",
    "\n",
    "For iteration in range(n_iterations):\n",
    "\n",
    "1. Do ...\n",
    "\n",
    "2. Do ...\n",
    "\n",
    "3. Do ...\n",
    "\n",
    "*Hint: Have you come across any numerical optimisers before?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple, though not very efficient way to train neural networks is to repeatedly sample random weights from a distribution of possible model weights and test whether any of them satisfy our objective using an objective function. In this case, the objective is to classify all points in the dataset correctly. So, the network should assign blue points with the class label \"0\" and orange points with the class label \"1\". A simple metric to quantify how well our network does this is to simply count the number of points that the network classifies incorrectly for a given set of model weights\n",
    "\n",
    "$$\n",
    "\\text{Loss}(w) = \\sum _{i=1}^n y_i(1-f(x_i, w)) + \\sum _{i=1}^n (1 - y_i)f(x_i, w).\n",
    "$$\n",
    "\n",
    "The loss function is used to measure how well the model's predictions match the actual labels in the dataset. \n",
    "The objective is to minimize this loss function, which means finding the set of model weights that makes the most accurate predictions.\n",
    "\n",
    "Here's a more intuitive breakdown of the equation:\n",
    "- $n$: the number of data points in the dataset\n",
    "- $y$: the actual class labels (either 0 or 1) for each data point\n",
    "- $f(x_i, w)$: the predicted class label (either 0 or 1) for the $i$-th data point, given the model weights $w$.\n",
    "- The two sums in the equation calculate the error for each data point in the dataset.\n",
    "- The first sum computes the error when the actual label is 1 but the model predicted it to be 0 (i.e. $f(x_i, w)=0$, so $1-f(x_i, w)=1$). In that case, the product of $y_i$ being 1 and $1-f(x_i, w)=1$ would count it as an error.\n",
    "- The first sum computes the error when the actual label is 0 but the model predicted it to be 1 (i.e. $f(x_i, w)=1$). In that case, the product of $1-y_i$ being 1 and $f(x_i, w)=1$ would count it as an error.\n",
    "- The combined loss is the sum of these individual errors for all data points in the dataset. The goal is to minimize this loss over all possible sets of model weights, which can be achieved using various optimization algorithms such as stochastic gradient descent.\n",
    "\n",
    "The optimisation algorithm would then repeatedly: \n",
    "1. sample model parameters; \n",
    "2. evaluate the objective function / loss for those model parameters; and \n",
    "3. accept the model parameters if the loss is lower than before. \n",
    "\n",
    "Let us test how well this stochastic optimisation algorithm does!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Implement the objective function / loss function\n",
    "You can use the implementation of the neural network below. Use the pre-implemented checks to test your solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We regenerate the data, just in case X was overwritten\n",
    "np.random.seed(0)\n",
    "n_samples = 50\n",
    "X = np.random.randn(n_samples, 2)\n",
    "y = np.zeros(n_samples)\n",
    "y[np.linalg.norm(X, axis=1) > 1] = 1\n",
    "\n",
    "def loss_function(weights):\n",
    "    y_pred = kai.neural_network(inputs=X.T, weights=weights)\n",
    "\n",
    "    # TODO Complete the implementation of the loss function\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([-2, 2, 1, -2, -1, 2, -2, -2, -1, -3, 2, -2])\n",
    "assert loss_function(weights) == 0\n",
    "\n",
    "weights += 1\n",
    "assert loss_function(weights) == 15\n",
    "\n",
    "weights -= 2\n",
    "assert loss_function(weights) == 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Train our neural network so it \"learns\" how to classify the points by itself\n",
    "\n",
    "The cell below implements the optimisation algortihm that we have discussed, using the loss function. Execute it to train the model. \n",
    "\n",
    "The code will produce two plots to visualise the training: \n",
    "1. the first plot shows the evolution of the loss over the iterations of the optimisation; and \n",
    "2. the second plot shows the final classification, learned by the neural network. \n",
    "\n",
    "Do the results make sense to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "np.random.seed(42)\n",
    "n_iterations = 1000\n",
    "losses = np.empty(n_iterations)\n",
    "for n in range(n_iterations):\n",
    "    # Sample proposal\n",
    "    proposal = np.random.uniform(low=-3, high=3, size=12)\n",
    "    loss = loss_function(proposal)\n",
    "\n",
    "    # Accept on first iteration\n",
    "    if n == 0:\n",
    "        weights = proposal\n",
    "        losses[n] = loss\n",
    "        continue\n",
    "\n",
    "    # Check whether loss is lower than for previous parameters\n",
    "    isProposalAccepted = loss < losses[n-1]\n",
    "    if isProposalAccepted:\n",
    "        weights = proposal\n",
    "        losses[n] = loss\n",
    "    else:\n",
    "        # We did not accept, so loss remains the same\n",
    "        losses[n] = losses[n-1]\n",
    "\n",
    "kai.plot_training_results(n_iterations, losses, weights, (X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is amazing! Our neural network learned to classify most of the data points all by itself! Why do you think did it not manage to classify all points perfectly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Other activation functions for better training\n",
    "\n",
    "In the above example, we trained a neural network using a simple optimisation algorithm that randomly samples weights from a distribution. This optimiser is guaranteed to evenutally find the optimal set of model parameters for our objective, the only problem is that it can take a veeeery large number of iterations to randomly sample this set of model parameters from the distribution. \n",
    "\n",
    "In practice, it is more common to update the neural network weights and biases more systematically. We do this by first randomly assigning weights and biases to the neural network. Why? Because we don't know where else to start. Our goal is to improve the parameters with each iteration, as we did before, but in a more systematic way.\n",
    "How? We run the network on each training example, compare the output and the desired output, and adjust the weights to minimise the error using the derivatives of the\n",
    "objective function.\n",
    "\n",
    "We want to find the weights and biases of the entire network, i.e. the parameters of the network. Starting from a random value, we need to see what the impact of changing a particular parameter is (e.g. $w_{1,1}$). We want a change in a parameter to lead to a change in the activation towards the desired activation.\n",
    "\n",
    "This is where we encounter our first problem with perceptrons: their activation function is a step function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [-1, 0, 1]\n",
    "y = [0, 0, 1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.step(x, y, color='red', linewidth=3)\n",
    "\n",
    "# Move the y-axis ticks to the center\n",
    "ax.spines['left'].set_position('center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Given we want to improve our weights in a systematic way, why is the step function a bad activation function for finding weights? (Discuss in your group).\n",
    "\n",
    "*Hint: How does the output of the activation function change for different inputs? And what does that mean for the gradients of the loss function?*\n",
    "\n",
    "When you are ready, execute the cell below for a visualisation of what is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Execute the cell below and use the slider in the figure to visualise how the step function changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kai.plot_step_activation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we change $w_1$ by $\\Delta w_1$, which will change $z$, which will impact $a = f(z)$. \n",
    "If we move the slider to the left, $z$ changes (red dot moves left), but the activation $a$ does not change: it was 0 before, but because $z < 0$, it remains unchanged at 0.\n",
    "If we move the slider slightly to the right, $a$ does not change either.\n",
    "We changed $w_1$ (and therefore $z$) in both ways and learned nothing about the impact of $w_1$.\n",
    "If you move the slider further to the right, all of a sudden $z > 0$ and $a = 1$.\n",
    "With the step function as the activation function, it's difficult to figure out how to adjust the weights and biases to get the desired activation/output.\n",
    "There is a lack of change or the sudden change in the output with a small change in the parameters.\n",
    "This is even more the case with a network of perceptrons.\n",
    "\n",
    "Since the step function is difficult to work with, we want to work with smoother functions, like the sigmoid function.\n",
    "\n",
    "*Aside: A mathematically more precise version of what we said above is: the gradient of the step function is 0 everywhere (except at 0, where it is infinite!), see right panel of the figure.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 The sigmoid activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid this problem (referred to as \"vanishing gradients\"), it is common to use other activation functions to improve the learning of neural networks. One of those activation functions is the sigmoid activation function illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kai.plot_sigmoid_activation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid activation function is almost the same as the step activation function that we have used before, the only differences is that the change from an output equal to 0 to an output equal to 1 now happens smoothly (avoiding the vashing gradients). Formally, the sigmoid activation can be written as\n",
    "\n",
    "$$\n",
    "a = \\sigma (z) = \\frac{1}{1 + e^{-z}}.\n",
    "$$\n",
    "\n",
    "In modern neural networks, many other activation functions are also used. Below we just show a random selection of these activation functions.\n",
    "\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
    "$$\n",
    "$$\n",
    "\\frac{d}{dx}\\tanh(x) = 1 - \\tanh^2(x)\n",
    "$$\n",
    "$$\n",
    "\\text{Swish}(x) = x \\cdot \\frac{1}{1 + e^{-\\beta x}}\n",
    "$$\n",
    "$$\n",
    "\\text{ELU}(x) = \\begin{cases} x, & \\text{if } x > 0 \\\\ \\alpha (e^x - 1), & \\text{otherwise} \\end{cases}\n",
    "$$\n",
    "\n",
    "Most importantly, they are all smooth, allowing a change in weights to result in a change in the activation, which allows us to systematically find neural network weights and biases (also called model parameters) that optimise the objective function using numerical optimisation algorithms, such as the gradient descent (GD) optimiser. A discussion of the GD optimiser is beyond the scope of today's workshop, but we will develop an inttuition for how it works by using it on different use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8: Train the neural network again using the GD algorithm\n",
    "\n",
    "Below, we have prepared an implementation of our neural networks using PyTorch (one of the most popular deep learning frameworks in Python). Execute the cell, and compare the result obtained with the GD optimiser to the previously obtained classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, n_inputs=2, n_perceptrons=4):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer = nn.Linear(n_inputs, n_perceptrons, dtype=torch.float64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Perceptron activation (now using sigmoid)\n",
    "        out = F.sigmoid(self.layer(x))\n",
    "\n",
    "        # Joint decision of perceptrons\n",
    "        out = torch.sum(out, axis=1)\n",
    "        y = F.sigmoid(out)\n",
    "\n",
    "        # A little hacky: y is now between 0.5 and 1. Recenter to 0 to 1\n",
    "        y = (y - 0.5) * 2\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimisation\n",
    "torch.manual_seed(8)\n",
    "model = NeuralNetwork()\n",
    "objective_function = nn.BCELoss()\n",
    "optimiser = optim.SGD(model.parameters(), lr=10)\n",
    "\n",
    "# We regenerate the data, just in case X was overwritten\n",
    "np.random.seed(0)\n",
    "n_samples = 50\n",
    "X = np.random.randn(n_samples, 2)\n",
    "y = np.zeros(n_samples)\n",
    "y[np.linalg.norm(X, axis=1) > 1] = 1\n",
    "inputs = torch.tensor(X)\n",
    "labels = torch.tensor(y)\n",
    "\n",
    "# Run optimisation\n",
    "losses = []\n",
    "n_iterations = 100\n",
    "for epoch in range(n_iterations):\n",
    "    # Reset the parameter gradients\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    # Simulation, evaluation and parameter proposal\n",
    "    outputs = model(inputs)\n",
    "    loss = objective_function(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    losses.append(loss.detach().numpy())\n",
    "\n",
    "kai.plot_results2(n_iterations, losses, model, (X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of incorrectly classified points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask0 = y == 0\n",
    "mask1 = y == 1\n",
    "y_pred = model(inputs).detach().numpy()\n",
    "n_misclassified = np.sum(1 - np.round(y_pred[mask1])) + np.sum(np.round(y_pred[mask0]))\n",
    "n_misclassified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GD optimiser finds a good set of network weights much faster than the stochastic optimiser, but it still fails to find the optimal arrangement of the decision boundaries -- even if we run the optimiser for many more iterations (try it!).\n",
    "\n",
    "One way to improve the performance of the network is to add more perceptrons to it. \n",
    "\n",
    "*(For the maths nerds among you: This creates more local minima which are able to classify the data points perfectly.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9: Retrain the network with more perceptrons\n",
    "\n",
    "Amend the network below, so that it uses 10 perceptrons for the classification instead of 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set the number of perceptrons and execute the cell\n",
    "n_perceptrons =\n",
    "\n",
    "# Set up optimisation\n",
    "torch.manual_seed(8)\n",
    "model = NeuralNetwork(n_perceptrons=n_perceptrons)\n",
    "objective_function = nn.BCELoss()\n",
    "optimiser = optim.SGD(model.parameters(), lr=10)\n",
    "\n",
    "# We regenerate the data, just in case X was overwritten\n",
    "np.random.seed(0)\n",
    "n_samples = 50\n",
    "X = np.random.randn(n_samples, 2)\n",
    "y = np.zeros(n_samples)\n",
    "y[np.linalg.norm(X, axis=1) > 1] = 1\n",
    "inputs = torch.tensor(X)\n",
    "labels = torch.tensor(y)\n",
    "\n",
    "# Run optimisation\n",
    "losses = []\n",
    "n_iterations = 100\n",
    "for epoch in range(n_iterations):\n",
    "    # Reset the parameter gradients\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    # Simulation, evaluation and parameter proposal\n",
    "    outputs = model(inputs)\n",
    "    loss = objective_function(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    losses.append(loss.detach().numpy())\n",
    "\n",
    "# Visualise loss over time\n",
    "iterations = np.arange(1, n_iterations+1)\n",
    "plt.plot(iterations, losses, linewidth=2)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss (cross-entropy)')\n",
    "plt.show()\n",
    "\n",
    "# Visualise the learned classification\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1])\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1])\n",
    "\n",
    "# Reshape model weights\n",
    "weights = np.empty(n_perceptrons*3)\n",
    "bias = model.layer.bias.detach().numpy()\n",
    "weights = model.layer.weight.detach().numpy()\n",
    "\n",
    "x1 = np.linspace(-3, 3, 100)\n",
    "for idx in range(n_perceptrons):\n",
    "    x2 = -(weights[idx, 0] * x1 + bias[idx]) / weights[idx, 1]\n",
    "    plt.plot(x1, x2, linestyle='-')\n",
    "\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-6, 6)\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of incorrectly classified points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask0 = y == 0\n",
    "mask1 = y == 1\n",
    "y_pred = model(inputs).detach().numpy()\n",
    "n_misclassified = np.sum(1 - np.round(y_pred[mask1])) + np.sum(np.round(y_pred[mask0]))\n",
    "n_misclassified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 From single-layer networks to multi-layer networks\n",
    "\n",
    "So far, when we added more perceptrons to the network, we treated all perceptrons equally, in the sense that all perceptrons process the raw input data and equally contribute to the final output of the neural network. This non-hierarchical use of perceptrons limits which tasks can be achieved by neural networks.\n",
    "\n",
    "In the final section of this notebook, we will begin to train multi-layer networks, introducing a hierarchy between the perceptrons. More specifically, we focus on a network architecture known as multi-layer perceptron (MLP). \n",
    "This MLP is part of almost all modern neural networks used today.\n",
    "\n",
    "To motivate the use of MLPs, let us try to use our simple multi-perceptron network from above to solve a more complex task: image classification. \n",
    "As an example for an image classification task, we use the digit dataset MNIST, which contains a large number of labelled images with digits in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10: Execute the cell below to illustrate some examples from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Load Data\n",
    "train_dataset = datasets.MNIST(root='dataset/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='dataset/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Visualise data\n",
    "images, labels = next(iter(train_loader))\n",
    "kai.plot_four_images(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of images (32x32 pixels) and corresponding labels. If we want to use our simple network from above to classify the images, we need it to take an image as input (i.e. 32x32 = 784 inputs) and output the class label. However, so far our model only predicts values between 0 and 1, making the mapping of the output to the class labels challenging.\n",
    "\n",
    "A common way to solve multi-class classification is to use a second layer that takes the inputs from the previous layer and processes them further to output several values -- one value for each class. If those outputs are normalised, they can be interpreted as the model's confidence for the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 11: Amend the single-layer perceptron to have 10 outputs instead of 1\n",
    "\n",
    "Use the second layer to process the output of the first layer and return 10 outputs. Test you implementation by training the model on the data using the cell below. If you implemented the model correctly, it should achieve a test accuracy of >95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork2(nn.Module):\n",
    "    def __init__(self, n_inputs=784, n_perceptrons=64, n_classes=10):\n",
    "        super(NeuralNetwork2, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_inputs, n_perceptrons)\n",
    "        self.layer2 = nn.Linear(n_perceptrons, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Perceptron activation (now using sigmoid)\n",
    "        out = F.relu(self.layer1(x))\n",
    "\n",
    "        # TODO: Process the output with the second layer\n",
    "        y =\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Network\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = NeuralNetwork2(\n",
    "    n_inputs=input_size, n_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch: {epoch}')\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        data = data.reshape(data.shape[0], -1)  # Flatten\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()  # set all gradients to zero for each batch\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "\n",
    "    if loader.dataset.train:\n",
    "        print(\"Accuracy on training data\")\n",
    "    else:\n",
    "        print(\"Accuracy on testing data\")\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():  # dont compute gradients\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            x = x.reshape(x.shape[0],-1)\n",
    "\n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "        print(f'Got {num_correct}/{num_samples} with accuracy {float(num_correct)/float(num_samples)*100: .2f}')\n",
    "\n",
    "    model.train()\n",
    "\n",
    "check_accuracy(train_loader, model)\n",
    "check_accuracy(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing! You just trained your first MLP and achieved an amazing image classification accuracy of >90% on a test dataset that the model has never seen before! So we really start to see how we can use networks of simple artificial neurons to achieve very complex tasks.\n",
    "\n",
    "In fact, one can prove that MLPs can lean to solve any task perfectly that can be formulated in terms of an input-output relationship, i.e. a function, and sufficient data is available. Here, the function that we are trying to learn from the data is the probability mass function over the digits conditional on the input image\n",
    "\n",
    "<img src=\"Picture_1.png\" width=\"350\" style=\"display: block; margin: 0 auto\">\n",
    "<img src=\"Picture_2.png\" width=\"350\" style=\"display: block; margin: 0 auto\">\n",
    "\n",
    "For some images, the digit is simple to identify and the model is expected to assign all probability mass to the corresponding digit class. For others, the handwriting might be more difficult to decipher and the probablity should be spread across the class labels more evenly.\n",
    "\n",
    "*Aside: Formally, the probablity mass function can be denoted by*\n",
    "$p(y| x) \\approx f(y | x, w)$,\n",
    "*where $y$ denotes the digit and $x$ denotes the image.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"multi_layer_perceptron_network.avif\" width=\"700\" style=\"display: block; margin: 0 auto\">\n",
    "\n",
    "While MLPs get closer to biological brains, in the sense that they use cascades of neurons to process information, it is certainly a stretch to expect MLPs to display intelligent behaviour. Nevertheless, a remarkable property of MLPs that lead to the brought success of AI in recent years is is that they are **universal function approximators**.\n",
    "\n",
    "Universal function approximation means that MLPs can perfectly describe any function, provided: 1. we have sufficient data from the function to train the model on; and 2. the MLP has sufficiently many artificial neurons in each layer. In this workshop, we will not go into the details of proving the universal function approximation theorem, but we built on MLPs to develop neural network architectures that can be used for natual language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Test the universal function approximation theorem\n",
    "\n",
    "Below is a mechanistic model of nerve membranes which models changes of the membrane's conductance over time -- these changes in the conductance contribute to our ability to feel touch, heat, or pain, among many other things. This model was developed by Fitzhugh in 1965 (see https://doi.org/10.1002/jcp.1030660518). The details of the model are not quite trivial, see e.g. https://pints.readthedocs.io/en/latest/toy/fitzhugh_nagumo_model.html. However, using neural networks and their ability to approximate any function, we can learn this complicated function using only input-output data points.\n",
    "\n",
    "Here, the input to the model is the time point at which we want to know the conductance of the nerve membrane, and the output is the corresponding action potential (voltage) on the membrane surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FitzhughNagumoModel()\n",
    "parameters = [0.1, 0.5, 3]\n",
    "times = np.linspace(0, 15, 200)\n",
    "voltage = model.simulate(parameters, times)[:, 0]\n",
    "\n",
    "# Plot simulation\n",
    "plt.figure()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.plot(times, voltage)\n",
    "plt.legend(['Voltage'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 12: Use a neural network to model the conductance in nerve membranes\n",
    "\n",
    "#### Step 1: Define an MLP with 1 input and 1 output\n",
    "You should use three linear layers (``nn.Linear``). Think about which activation functions make sense for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the neural network\n",
    "class NeuralNetwork3(nn.Module):\n",
    "    def __init__(self, n_inputs=1, n_perceptrons=200, n_outputs=1):\n",
    "        super(NeuralNetwork3, self).__init__()\n",
    "        # TODO: Define the layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Define how the layers process the information\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Train the neural network on the data\n",
    "\n",
    "We have pre-implemented a training script for you below. The only thing left for your to do is to choose an objective function.\n",
    "\n",
    "Which objective function makes sense for the training? You can find all pre-implemented objective functions in PyTorch here: https://pytorch.org/docs/stable/nn.html#loss-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define a suitable objective function\n",
    "objective_function =\n",
    "\n",
    "# Set up optimisation\n",
    "torch.manual_seed(8)\n",
    "model = NeuralNetwork3()\n",
    "optimiser = optim.SGD(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Format data\n",
    "inputs = torch.tensor(times[:, np.newaxis])\n",
    "true_outputs = torch.tensor(voltage[:, np.newaxis])\n",
    "n = len(inputs)\n",
    "\n",
    "# Run optimisation\n",
    "n_samples = 5\n",
    "losses = []\n",
    "n_iterations = 300000\n",
    "for epoch in range(n_iterations):\n",
    "    # Reset the parameter gradients\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    # Take subset of data\n",
    "    idx = np.random.randint(n, size=n_samples)\n",
    "    t = inputs[idx]\n",
    "    o = true_outputs[idx]\n",
    "\n",
    "    # Simulation, evaluation and parameter proposal\n",
    "    outputs = model(t)\n",
    "    loss = objective_function(outputs, o)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    losses.append(loss.detach().numpy())\n",
    "\n",
    "# Plot loss\n",
    "iterations = np.arange(1, n_iterations+1)\n",
    "plt.plot(iterations, losses, linewidth=2)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Objective function')\n",
    "plt.show()\n",
    "\n",
    "pred = model(inputs).detach().numpy()[:, 0]\n",
    "plt.plot(times, pred, linewidth=2, label='Neural network')\n",
    "plt.plot(times, voltage, linewidth=2, label='True', linestyle='--')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Voltage')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, the neural network should provide a close approximation of the true membrane conductance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we provide an intuitive introduction to the foundations of artificial intelligence.\n",
    "\n",
    "**Take-aways:**\n",
    "By the end of this notebook, you will have seen:\n",
    "- how biological brains inspired the development of artificial neural networks in the hope that models of the brain would display emerging intelligent behaviour;\n",
    "- how multi-layer perceptrons (MLPs) provide the backbone of all modern neural networks;\n",
    "- and how MLPs can approximate any function (universal function approximation theorem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, FloatSlider\n",
    "import numpy as np\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The Perceptron\n",
    "\n",
    "Intuitively, one might expect artificial intelligence (AI) to arise when detailed models of the human brain are simulated on a computer. This intuition motivated the development of artificial neural networks more than 60 years ago. These artificial neural networks are now at the core of all modern AI models. In this notebook, we will retrace these early developments of artificial intelligence to better understand why language models, such as GPT3, process language the way they do.\n",
    "\n",
    "The smallest building blocks of brains are brain cells, also known as neurons. Each neuron by itself is not more intelligent than any other cell in our body. But our brains are composed of approximately 100 billion neurons which communicate with each other and process information in a coordinated way. This is widely believed to lead to the emergence of intelligence. A good start for artificial intelligence is therefore to first build a model of a neuron, and then make many of those neurons communicate in the hope to see artificial intelligence to emerge in a similar way to biological neural networks.\n",
    "\n",
    "In the figure below, we illustrate a human neuron in the left panel. The right panel shows a model of the neuron: an artificial neuron, commonly referred to as *perceptron*. The biological neuron consists of a junction of dendrites with the cell nucleus in the middle, and an axon leading away from the neuron. Neurons receive electrical signals through their dendrites and forward strong enough signals to other neurons along their axons. The perceptron emulates this behaviour by receiving $n$ input signals $x_1, \\ldots, x_n$ which are processed into one output signal $y$.\n",
    "\n",
    "<img src=\"biological-neuron.jpg\" width=\"500\" style=\"display: block; margin: 0 auto\" >\n",
    "\n",
    "More specifically, we may define a perceptron using a linear combination of the input signals and a step function that only returns a value of $1$ when the weighted sum is greater than zero \n",
    "\n",
    "$$\n",
    "y = f(x, w) = \\Theta (w_0\\, x_0 + \\ldots + w_n\\, x_n),\n",
    "$$\n",
    "where $\\Theta(z) = 1$ for $z>0$, and $\\Theta(y) = 0$ otherwise. This activation function, $\\Theta(z)$, mimics the behaviour of neurons to only forward signals if the incoming signals are large enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement a simple artificial neuron\n",
    "\n",
    "To familiarise yourselfves with how perceptrons process information, implement a perceptron by completing the code block below. When you are ready, use the notebook cell after that to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(x):\n",
    "    \"\"\"\n",
    "    Returns the output of a perceptron with 6 input signals.\n",
    "\n",
    "    Parameters:\n",
    "        x (List of length 6) Input signals to the perceptron.\n",
    "    \"\"\"\n",
    "    weights = [0.2, -0.3, 0.9, 0.4, -0.9, -0.5]\n",
    "\n",
    "    # TODO: Calculate y\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">TODO:</span> Need simple checks of the solutions where the function is checked against the correct outputs? Last time I did this for example by pre-implementing the correct solution in a module, see In [6] in https://github.com/equinor/edc2023-optimising-wind-farms/blob/main/03_wind_farm_model/solution.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 From single perceptrons to networks of perceptrons\n",
    "\n",
    "Without activation functions, single perceptrons are mathematically equivalent to linear regression models (which have been used in the machine learning community already for a very long time)\n",
    "\n",
    "$$\n",
    "    z = \\sum _{j=1}^n w_j x_j.\n",
    "$$\n",
    "\n",
    "For just two inputs, this makes it possible to visualise the outputs of perceptrons for different input values using straight lines in a plane, see below. Note that it is convential to consider $x_0$ to be fixed to one, $x_0=1$. This enables translations of the lines in the plane. The corresponding weight, $w_0$, is commonly referred to as the bias of the perceptron. \n",
    "\n",
    "<img src=\"linear_classifier.png\" width=\"500\" style=\"display: block; margin: 0 auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation of perceptrons as binary linear regressors, i.e. linear classifiers, makes intuitively clear how the smallest building blocks of artificial neural networks process information. But one perceptron alone is limited in what it can do and certainly does not achieve artificial \"intelligence\".\n",
    "\n",
    "To visualise the limitations of a single perceptron, let us attempt a simple 2-dimensional classification task using the perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random data\n",
    "np.random.seed(0)\n",
    "n_samples = 50\n",
    "X = np.random.randn(n_samples, 2)\n",
    "y = np.zeros(n_samples)\n",
    "y[np.linalg.norm(X, axis=1) > 1] = 1\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], label='Class 1')\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], label='Class 2')\n",
    "\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-6, 6)\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the task for the perceptron is to classify the points based on the $x_1$ and $x_2$ coordinates. It is clear that we will not be able to use just a single line to separate the blue dots from the orange dots. But, we could, perhaps, use 4 perceptrons to classify the data..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Find 4 perceptrons that can jointly classify the data points\n",
    "For example, you could decide that a data point is classified as \"blue\" if all perceptrons return a 1. Otherwise the data point is classified as orange. You can use the below code block to define the weights of the perceptrons, and the code block after that to visualise their decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the dictionaries to define weights and biases of the perceptrons\n",
    "perceptron1 = {\n",
    "    'bias': -2,\n",
    "    'weight1': 2,\n",
    "    'weight2': 1\n",
    "}\n",
    "perceptron2 = {\n",
    "    'bias': ,\n",
    "    'weight1': ,\n",
    "    'weight2':\n",
    "}\n",
    "perceptron3 = {\n",
    "    'bias': ,\n",
    "    'weight1': ,\n",
    "    'weight2':\n",
    "}\n",
    "perceptron4 = {\n",
    "    'bias': ,\n",
    "    'weight1': ,\n",
    "    'weight2':\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1])\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1])\n",
    "\n",
    "# linear classifications\n",
    "x1 = np.linspace(-3, 3, 100)\n",
    "x2 = -(perceptron1['weight1'] * x1 + perceptron1['bias']) / perceptron1['weight2']\n",
    "plt.plot(x1, x2, linestyle='-', color='red')\n",
    "\n",
    "x2 = -(perceptron2['weight1'] * x1 + perceptron2['bias']) / perceptron2['weight2']\n",
    "plt.plot(x1, x2, linestyle='-', color='orange')\n",
    "\n",
    "x2 = -perceptron3['weight1'] / perceptron3['weight2'] * x1 - perceptron3['bias'] / perceptron3['weight2']\n",
    "plt.plot(x1, x2, linestyle='-', color='lightblue')\n",
    "\n",
    "x2 = -perceptron4['weight1'] / perceptron4['weight2'] * x1 - perceptron4['bias'] / perceptron4['weight2']\n",
    "plt.plot(x1, x2, linestyle='-', color='darkcyan')\n",
    "\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-6, 6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise demonstrates that networks of artificial neurons can perform more complex tasks than single perceptrons. The figure below illustrates this network diagrammatically.\n",
    "\n",
    "<img src=\"complex_classifier_perceptron_diagram.png\" width=\"1000\" style=\"display: block; margin: 0 auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, so far our neural network does not feel very \"intelligent\". It can only classify points in a plane, and we had to painfully tweak the weights of the network to do it. More complex networks are able to perform more complex tasks (modern neural networks, such as large language models, have up to 200 billion model weights), but we need to find a way to avoid manually tweaking the weights of network networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Training neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The automatic tweaking of neural network weights is achieved using numerical optimisation algorithms and is referred to as \"training\" or as \"learning\" in the AI community. While \"training\" and \"learning\" sounds fancy, the high level concept of automatically tweaking network weights to optimise an objective is quite intuitive..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Discuss in your group how you would design an iterative 3-step algorithm that can optimise neural network weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the below algorithm:\n",
    "\n",
    "For iteration in range(n_iterations):\n",
    "\n",
    "1. Do ... Propose weights\n",
    "\n",
    "2. Do ... Evaluate objective\n",
    "\n",
    "3. Do ... Accept or reject weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you come across any numerical optimisers before?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple, though not very efficient way to train neural networks is to repeatedly sample random weights from a distribution of possible model weights and test whether any of them satisfy our objective using an objective function. In this case, the objective is to classify all points in the dataset correctly. So, the network should assign blue points with the class label \"0\" and orange points with the class label \"1\". A simple metric to quantify how well our network does this is to simply count the number of points that the network classifies incorrectly for a given set of model weights\n",
    "\n",
    "$$\n",
    "\\text{Loss}(w) = \\sum _{j=1}^n y_j(1-f(x_i, w)) + \\sum _{j=1}^n (1 - y_j)f(x_i, w).\n",
    "$$\n",
    "\n",
    "The optimisation algorithm would then repeatedly: 1. sample model parameters; 2. evaluate the objective function / loss for those model parameters; and 3. accept the model parameters if the loss is lower than before. Let us test how well this stochastic optimisation algorithm does!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Implement the objective function / loss function\n",
    "You can use the implementation of the neural network below. Use the pre-implemented checks to test your solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(inputs, weights):\n",
    "    \"\"\"\n",
    "    Returns the output of a perceptron with 3 input signals.\n",
    "\n",
    "    Parameters:\n",
    "        inputs (np.ndarray of shape (2, n)) Input data.\n",
    "        weights (np.ndarray of shape (3,)) Weights of the network.\n",
    "    \"\"\"\n",
    "    # Calculate linear output\n",
    "    z = weights[0] + weights[1] * inputs[0] + weights[2] * inputs[1]\n",
    "\n",
    "    # Calculate activation\n",
    "    y = np.array(z > 0, dtype=float)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def neural_network(inputs, weights):\n",
    "    \"\"\"\n",
    "    Returns the output of the simple neural network.\n",
    "\n",
    "    Parameters:\n",
    "        inputs (np.ndarray of shape (2, n)) Input data.\n",
    "        weights (np.ndarray of shape (12,)) Weights of the network.\n",
    "    \"\"\"\n",
    "    inputs = np.array(inputs)\n",
    "    weights = np.array(weights)\n",
    "\n",
    "    # Parse weights\n",
    "    weights1 = weights[:3]\n",
    "    weights2 = weights[3:6]\n",
    "    weights3 = weights[6:9]\n",
    "    weights4 = weights[9:12]\n",
    "\n",
    "    # Process data\n",
    "    n = inputs.shape[1]\n",
    "    out = np.empty(shape=(4, n))\n",
    "    for idw, w in enumerate([weights1, weights2, weights3, weights4]):\n",
    "        out[idw] = perceptron(inputs, w)\n",
    "\n",
    "    # If all perceptrons predict a 0, the label is a zero. Otherwise 1\n",
    "    y = np.sum(out, axis=0)\n",
    "    y[y > 0] = 1\n",
    "\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We regenerate the data, just in case X was overwritten\n",
    "np.random.seed(0)\n",
    "n_samples = 50\n",
    "X = np.random.randn(n_samples, 2)\n",
    "y = np.zeros(n_samples)\n",
    "y[np.linalg.norm(X, axis=1) > 1] = 1\n",
    "\n",
    "def loss_function(weights):\n",
    "    # TODO: Implement the loss function\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([-2, 2, 1, -2, -1, 2, -2, -2, -1, -3, 2, -2])\n",
    "assert loss_function(weights) == 0\n",
    "\n",
    "weights += 1\n",
    "assert loss_function(weights) == 15\n",
    "\n",
    "weights -= 2\n",
    "assert loss_function(weights) == 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Train our neural network so it \"learns\" how to classify the points by itself\n",
    "\n",
    "The cell below implements the optimisation algortihm that we have discussed, using the loss function. Execute it to train the model. \n",
    "\n",
    "The code will produce plots to visualise the training: 1. the evolution of the loss over the iterations of the optimisation; and 2. the final classification, learned by the neural network. Do the results make sense to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "np.random.seed(42)\n",
    "n_iterations = 1000\n",
    "losses = np.empty(n_iterations)\n",
    "for n in range(n_iterations):\n",
    "    # Sample proposal\n",
    "    proposal = np.random.uniform(low=-3, high=3, size=12)\n",
    "    loss = loss_function(proposal)\n",
    "\n",
    "    # Accept on first iteration\n",
    "    if n == 0:\n",
    "        weights = proposal\n",
    "        losses[n] = loss\n",
    "        continue\n",
    "\n",
    "    # Check whether loss is lower than for previous parameters\n",
    "    isProposalAccepted = loss < losses[n-1]\n",
    "    if isProposalAccepted:\n",
    "        weights = proposal\n",
    "        losses[n] = loss\n",
    "    else:\n",
    "        # We did not accept, so loss remains the same\n",
    "        losses[n] = losses[n-1]\n",
    "\n",
    "\n",
    "# Visualise loss over time\n",
    "iterations = np.arange(1, n_iterations+1)\n",
    "plt.plot(iterations, losses, linewidth=2)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss (number of misclassifications)')\n",
    "plt.show()\n",
    "\n",
    "# Visualise the learned classification\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1])\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1])\n",
    "\n",
    "x1 = np.linspace(-3, 3, 100)\n",
    "x2 = -(weights[1] * x1 + weights[0]) / weights[2]\n",
    "plt.plot(x1, x2, linestyle='-', color='red')\n",
    "x2 = -(weights[4] * x1 + weights[3]) / weights[5]\n",
    "plt.plot(x1, x2, linestyle='-', color='orange')\n",
    "x2 = -(weights[7] * x1 + weights[6]) / weights[8]\n",
    "plt.plot(x1, x2, linestyle='-', color='lightblue')\n",
    "x2 = -(weights[10] * x1 + weights[9]) / weights[11]\n",
    "plt.plot(x1, x2, linestyle='-', color='darkcyan')\n",
    "\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-6, 6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is amazing! Our neural network learned to classify most of the data points all by itself! Why do you think did it not manage to classify all points perfectly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Other activation functions for better training\n",
    "\n",
    "In the above example, we trained a neural network using a simple optimisation algorithm that randomly samples weights from a distribution. This optimiser is guaranteed to evenutally find the optimal set of model parameters for our objective, the only problem is that it can take a veeeery large number of iterations to randomly sample this set of model parameters from the distribution. Common practice is therefore to use more advanced optimisers that take more information from the loss function into account to solve the optimisation task.\n",
    "\n",
    "The most popular among those optimisers is the gradient descent (GD) algorithm which uses local derivatives of the loss function to step into the direction of its steepest descent. We will use pre-implemented versions of the GD algorithm in the remainder of this workshop \n",
    "\n",
    "However, before we do so, we still have one amendment to make to the action functions that we have used so far. Can you think of the reason why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aside:** Note that the GD optimiser is no longer guaranteed to find the globally optimal network weights. Discussing this is beyond this workshop. But if you are interested, have a look at this optimisation workshop: https://github.com/equinor/edc2023-optimising-wind-farms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Why will a GD algorithm not be able to optimise our neural network?\n",
    "\n",
    "Look at the visualisation of the activation function below and discuss in your group why the GD algorithm might struggle to optimise networks that use this activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [-1, 0, 1]\n",
    "y = [0, 0, 1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.step(x, y, color='red', linewidth=3)\n",
    "\n",
    "# Move the y-axis ticks to the center\n",
    "ax.spines['left'].set_position('center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: How does the output of the activation function change for different inputs? And what does that mean for the gradients of the loss function?\n",
    "\n",
    "When you are ready, execute the cell below for a visualisation of the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Execute the cell below and use the slider in the figure to visualise how the gradient changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def step_function_gradient(x):\n",
    "    return 0\n",
    "\n",
    "# Define the function to plot\n",
    "def plot_sine(delta_z):\n",
    "    my_dpi = 192\n",
    "    fig = plt.figure(figsize=(2250 // my_dpi, 900 // my_dpi), dpi=150)\n",
    "    outer = gridspec.GridSpec(1, 2, hspace=0.6)\n",
    "\n",
    "    # Create axes\n",
    "    axes = []\n",
    "    axes.append(plt.Subplot(fig, outer[0]))\n",
    "    axes.append(plt.Subplot(fig, outer[1]))\n",
    "\n",
    "    # Add axes to figure\n",
    "    for ax in axes:\n",
    "        fig.add_subplot(ax)\n",
    "\n",
    "    # plot step function\n",
    "    x = [-0.55, 0, 0.45]\n",
    "    y = [0, 0, 1]\n",
    "    axes[0].step(x, y, color='black', linewidth=3)\n",
    "    axes[1].plot(x, [0, 0, 0], color='black', linewidth=3)\n",
    "\n",
    "    # plot the starting point\n",
    "    z0 = -0.05\n",
    "    z = z0 + delta_z\n",
    "    axes[0].plot(z0, step_function(z0), 'ro', markerfacecolor='none')\n",
    "    axes[0].text(z0, step_function(z0) - 0.2, rf'$z_0 = {z0}$', fontsize=12, ha='center')\n",
    "    axes[1].plot(z0, step_function_gradient(z0), 'ro', markerfacecolor='none')\n",
    "    axes[1].text(z0, step_function_gradient(z0) - 0.2, rf'$z_0 = {z0}$', fontsize=12, ha='center')\n",
    "\n",
    "    # plot the moving point\n",
    "    axes[0].plot(z, step_function(z), 'ro')\n",
    "    axes[0].text(z, step_function(z) + 0.2, rf'$z = z_0 + \\Delta z = {z:.2f}$', fontsize=12, ha='right')\n",
    "    axes[1].plot(z, step_function_gradient(z), 'ro')\n",
    "    axes[1].text(z, step_function_gradient(z) + 0.2, rf'$z = z_0 + \\Delta z = {z:.2f}$', fontsize=12, ha='right')\n",
    "\n",
    "    axes[0].set_ylim([-1, 2])\n",
    "    axes[0].set_xlabel(r'$z = \\mathbf{w \\cdot x}$')\n",
    "    axes[0].set_ylabel(r'$f(z)$')\n",
    "    axes[0].set_title('Output of perceptron')\n",
    "\n",
    "    axes[1].set_ylim([-1, 2])\n",
    "    axes[1].set_xlabel(r'$z = \\mathbf{w \\cdot x}$')\n",
    "    axes[1].set_ylabel(r'$\\partial f(z) / \\partial w_j$')\n",
    "    axes[1].set_title('Gradient of perceptron')\n",
    "    plt.show()\n",
    "\n",
    "# Create the slider\n",
    "slider = FloatSlider(min=-0.15, max=0.5, step=0.01, value=0)\n",
    "\n",
    "# Create the interactive plot\n",
    "func = interact(plot_sine, delta_z=slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the activation function with respect to the model weights is practically zero everywhere (it is infinite exactly at z = 0)! As a result, also the gradient of the loss function with respect to the weights is practically zero everywhere. This makes the use of the GD algorithm for our network impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 The sigmoid activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid this \"vanishing gradients\" problem, it is common to use other activation functions to improve the learning of neural networks. One of those activation functions is the sigmoid activation function illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_gradient(x):\n",
    "    return np.exp(-x) / (1 + np.exp(-x))**2\n",
    "\n",
    "# Define the function to plot\n",
    "def plot_sine(delta_z):\n",
    "    my_dpi = 192\n",
    "    fig = plt.figure(figsize=(2250 // my_dpi, 900 // my_dpi), dpi=150)\n",
    "    outer = gridspec.GridSpec(1, 2, hspace=0.6)\n",
    "\n",
    "    # Create axes\n",
    "    axes = []\n",
    "    axes.append(plt.Subplot(fig, outer[0]))\n",
    "    axes.append(plt.Subplot(fig, outer[1]))\n",
    "\n",
    "    # Add axes to figure\n",
    "    for ax in axes:\n",
    "        fig.add_subplot(ax)\n",
    "\n",
    "    # Plot reference\n",
    "    x = np.linspace(-20, 20, 100)\n",
    "    axes[0].plot(x, sigmoid(x), color='black', linewidth=3)\n",
    "    axes[1].plot(x, sigmoid_gradient(x), color='black', linewidth=3)\n",
    "\n",
    "    # plot the starting point\n",
    "    z0 = -5\n",
    "    z = z0 + delta_z\n",
    "    axes[0].plot(z0, sigmoid(z0), 'ro', markerfacecolor='none')\n",
    "    axes[0].text(z0, sigmoid(z0) - 0.2, rf'$z_0 = {z0}$', fontsize=12, ha='center')\n",
    "    axes[1].plot(z0, sigmoid_gradient(z0), 'ro', markerfacecolor='none')\n",
    "    axes[1].text(z0, sigmoid_gradient(z0) + 0.02, rf'$z_0 = {z0}$', fontsize=12, ha='center')\n",
    "\n",
    "    # plot the moving point\n",
    "    axes[0].plot(z, sigmoid(z), 'ro')\n",
    "    axes[0].text(z, sigmoid(z) + 0.2, rf'$z = z_0 + \\Delta z = {z:.2f}$', fontsize=12, ha='right')\n",
    "    axes[1].plot(z, sigmoid_gradient(z), 'ro')\n",
    "    axes[1].text(z, sigmoid_gradient(z) + 0.02, rf'$z = z_0 + \\Delta z = {z:.2f}$', fontsize=12, ha='right')\n",
    "\n",
    "    axes[0].set_ylim([-0.5, 1.5])\n",
    "    axes[0].set_title(rf'Impact of changing $z$ on activation')\n",
    "    plt.show()\n",
    "\n",
    "# Create the slider\n",
    "slider = FloatSlider(min=-7, max=15, step=0.1, value=0)\n",
    "\n",
    "# Create the interactive plot\n",
    "func = interact(plot_sine, delta_z=slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid activation function is almost the same as the step activation function that we have used before, the only differences is that the change from an output equal to 0 to an output equal to 1 now happens smoothly, avoiding the vashing gradients (at least for z-values close to 0). Formally, the sigmoid activation can be written as\n",
    "\n",
    "$$\n",
    "a = \\sigma (z) = \\frac{1}{1 + e^{-z}}.\n",
    "$$\n",
    "\n",
    "In modern neural networks, many other activation functions are also used. Below we just show a random selection of these activation functions.\n",
    "\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
    "$$\n",
    "$$\n",
    "\\frac{d}{dx}\\tanh(x) = 1 - \\tanh^2(x)\n",
    "$$\n",
    "$$\n",
    "\\text{Swish}(x) = x \\cdot \\frac{1}{1 + e^{-\\beta x}}\n",
    "$$\n",
    "$$\n",
    "\\text{ELU}(x) = \\begin{cases} x, & \\text{if } x > 0 \\\\ \\alpha (e^x - 1), & \\text{otherwise} \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8: Train the neural network again using the GD algorithm\n",
    "\n",
    "Below, we have prepared an implementation of our neural networks using PyTorch (one of the most popular deep learning frameworks in Python). Execute the cell, and compare the result obtained with the GD optimiser to the previously obtained classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, n_inputs=2, n_perceptrons=4):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer = nn.Linear(n_inputs, n_perceptrons, dtype=torch.float64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Perceptron activation (now using sigmoid)\n",
    "        out = F.sigmoid(self.layer(x))\n",
    "\n",
    "        # Joint decision of perceptrons\n",
    "        out = torch.sum(out, axis=1)\n",
    "        y = F.sigmoid(out)\n",
    "\n",
    "        # A little hacky: y is now between 0.5 and 1. Recenter to 0 to 1\n",
    "        y = (y - 0.5) * 2\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimisation\n",
    "torch.manual_seed(8)\n",
    "model = NeuralNetwork()\n",
    "objective_function = nn.BCELoss()\n",
    "optimiser = optim.SGD(model.parameters(), lr=10)\n",
    "\n",
    "# We regenerate the data, just in case X was overwritten\n",
    "np.random.seed(0)\n",
    "n_samples = 50\n",
    "X = np.random.randn(n_samples, 2)\n",
    "y = np.zeros(n_samples)\n",
    "y[np.linalg.norm(X, axis=1) > 1] = 1\n",
    "inputs = torch.tensor(X)\n",
    "labels = torch.tensor(y)\n",
    "\n",
    "# Run optimisation\n",
    "losses = []\n",
    "n_iterations = 100\n",
    "for epoch in range(n_iterations):\n",
    "    # Reset the parameter gradients\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    # Simulation, evaluation and parameter proposal\n",
    "    outputs = model(inputs)\n",
    "    loss = objective_function(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    losses.append(loss.detach().numpy())\n",
    "\n",
    "# Visualise loss over time\n",
    "iterations = np.arange(1, n_iterations+1)\n",
    "plt.plot(iterations, losses, linewidth=2)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss (cross-entropy)')\n",
    "plt.show()\n",
    "\n",
    "# Visualise the learned classification\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1])\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1])\n",
    "\n",
    "# Reshape model weights\n",
    "weights = np.empty(12)\n",
    "bias = model.layer.bias.detach().numpy()\n",
    "weights = model.layer.weight.detach().numpy()\n",
    "\n",
    "x1 = np.linspace(-3, 3, 100)\n",
    "x2 = -(weights[0, 0] * x1 + bias[0]) / weights[0, 1]\n",
    "plt.plot(x1, x2, linestyle='-', color='red')\n",
    "x2 = -(weights[1, 0] * x1 + bias[1]) / weights[1, 1]\n",
    "plt.plot(x1, x2, linestyle='-', color='orange')\n",
    "x2 = -(weights[2, 0] * x1 + bias[2]) / weights[2, 1]\n",
    "plt.plot(x1, x2, linestyle='-', color='lightblue')\n",
    "x2 = -(weights[3, 0] * x1 + bias[3]) / weights[3, 1]\n",
    "plt.plot(x1, x2, linestyle='-', color='darkcyan')\n",
    "\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-6, 6)\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of incorrectly classified points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask0 = y == 0\n",
    "mask1 = y == 1\n",
    "y_pred = model(inputs).detach().numpy()\n",
    "n_misclassified = np.sum(1 - np.round(y_pred[mask1])) + np.sum(np.round(y_pred[mask0]))\n",
    "n_misclassified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GD optimiser finds a good set of network weights much faster than the stochastic optimiser, but it still fails to find the optimal arrangement of the decision boundaries -- even if we run the optimiser for many more iterations (try it!). Can you think of a reason why? \n",
    "\n",
    "One way to improve the performance of the network is to add more perceptrons to it. This creates more local minima which are able to classify the data points perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9: Retrain the network with more perceptrons\n",
    "\n",
    "Amend the network below, so that it uses 10 perceptrons for the classification instead of 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set the number of perceptrons and execute the cell\n",
    "n_perceptrons =\n",
    "\n",
    "# Set up optimisation\n",
    "torch.manual_seed(8)\n",
    "model = NeuralNetwork(n_perceptrons=n_perceptrons)\n",
    "objective_function = nn.BCELoss()\n",
    "optimiser = optim.SGD(model.parameters(), lr=10)\n",
    "\n",
    "# We regenerate the data, just in case X was overwritten\n",
    "np.random.seed(0)\n",
    "n_samples = 50\n",
    "X = np.random.randn(n_samples, 2)\n",
    "y = np.zeros(n_samples)\n",
    "y[np.linalg.norm(X, axis=1) > 1] = 1\n",
    "inputs = torch.tensor(X)\n",
    "labels = torch.tensor(y)\n",
    "\n",
    "# Run optimisation\n",
    "losses = []\n",
    "n_iterations = 100\n",
    "for epoch in range(n_iterations):\n",
    "    # Reset the parameter gradients\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    # Simulation, evaluation and parameter proposal\n",
    "    outputs = model(inputs)\n",
    "    loss = objective_function(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    losses.append(loss.detach().numpy())\n",
    "\n",
    "# Visualise loss over time\n",
    "iterations = np.arange(1, n_iterations+1)\n",
    "plt.plot(iterations, losses, linewidth=2)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss (cross-entropy)')\n",
    "plt.show()\n",
    "\n",
    "# Visualise the learned classification\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1])\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1])\n",
    "\n",
    "# Reshape model weights\n",
    "weights = np.empty(n_perceptrons*3)\n",
    "bias = model.layer.bias.detach().numpy()\n",
    "weights = model.layer.weight.detach().numpy()\n",
    "\n",
    "x1 = np.linspace(-3, 3, 100)\n",
    "for idx in range(n_perceptrons):\n",
    "    x2 = -(weights[idx, 0] * x1 + bias[idx]) / weights[idx, 1]\n",
    "    plt.plot(x1, x2, linestyle='-')\n",
    "\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-6, 6)\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of incorrectly classified points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask0 = y == 0\n",
    "mask1 = y == 1\n",
    "y_pred = model(inputs).detach().numpy()\n",
    "n_misclassified = np.sum(1 - np.round(y_pred[mask1])) + np.sum(np.round(y_pred[mask0]))\n",
    "n_misclassified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 From single-layer networks to multi-layer networks\n",
    "\n",
    "In the final section of this notebook, we will begin to train multi-layer networks. More specifically, we focus on a network architecture known as multi-layer perceptron (MLP). This MLP is part of almost all modern neural networks used today.\n",
    "\n",
    "To motivate the use of MLPs, let us try to use our simple multi-perceptron network from above to solve a more complex task: image classification. As an example for an image classification task, we use the digit dataset MNIST, which contains a large number of labelled images with digits in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10: Execute the cell below to illustrate some examples from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Load Data\n",
    "train_dataset = datasets.MNIST(root='dataset/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='dataset/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Visualise data\n",
    "# Create layout\n",
    "fontsize = 14\n",
    "my_dpi = 192\n",
    "fig = plt.figure(figsize=(2250 // my_dpi, 1200 // my_dpi), dpi=150)\n",
    "outer = gridspec.GridSpec(1, 4)\n",
    "\n",
    "# Create axes\n",
    "axes = []\n",
    "axes.append(plt.Subplot(fig, outer[0]))\n",
    "axes.append(plt.Subplot(fig, outer[1]))\n",
    "axes.append(plt.Subplot(fig, outer[2]))\n",
    "axes.append(plt.Subplot(fig, outer[3]))\n",
    "\n",
    "# Add axes to figure\n",
    "for ax in axes:\n",
    "    fig.add_subplot(ax)\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "for ida, ax in enumerate(axes):\n",
    "    ax.imshow(images[ida][0], cmap='gray')\n",
    "    ax.set_title('Label: %d' % labels[ida])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of images (32x32 pixels) and corresponding labels. If we want to use our simple network from above to classify the images, we need it to take an image as input (i.e. 32x32 = 784 input signals) and output the class label. However, so far out model only predicts values between 0 and 1, making the mapping of the output to the class labels challenging.\n",
    "\n",
    "A common way to solve multi-class classification is to use a second layer that takes the inputs from the previous layer and processes them to several outputs -- one output for each class. If those outputs are normalised, they can be interpreted as the model's confidence for the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 11: Amend the single-layer perceptron to have 10 outputs instead of 1\n",
    "\n",
    "Use the second layer to process the output of the first layer and return 10 outputs. Test you implementation by training the model on the data using the cell below. If you implemented the model correctly, it should achieve a test accuracy of >95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork2(nn.Module):\n",
    "    def __init__(self, n_inputs=784, n_perceptrons=64, n_classes=10):\n",
    "        super(NeuralNetwork2, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_inputs, n_perceptrons)\n",
    "        self.layer2 = nn.Linear(n_perceptrons, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Perceptron activation (now using sigmoid)\n",
    "        out = F.relu(self.layer1(x))\n",
    "\n",
    "        # TODO: Process the output with the second layer\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Network\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = NeuralNetwork2(\n",
    "    n_inputs=input_size, n_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch: {epoch}')\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        data = data.reshape(data.shape[0], -1)  # Flatten\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()  # set all gradients to zero for each batch\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "\n",
    "    if loader.dataset.train:\n",
    "        print(\"Accuracy on training data\")\n",
    "    else:\n",
    "        print(\"Accuracy on testing data\")\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():  # dont compute gradients\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            x = x.reshape(x.shape[0],-1)\n",
    "\n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "        print(f'Got {num_correct}/{num_samples} with accuracy {float(num_correct)/float(num_samples)*100: .2f}')\n",
    "\n",
    "    model.train()\n",
    "\n",
    "check_accuracy(train_loader, model)\n",
    "check_accuracy(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing! You just trained your first MLP and achieved an amazing image classification accuracy of >95% on a test dataset that the model has never seen before! So we really start to see how we can use networks of simple artificial neurons to achieve very complex tasks.\n",
    "\n",
    "In fact, one can prove that MLPs can lean to solve any task perfectly that can be formulated in terms of an input-output relationship, i.e. a function, and sufficient data is available. Here, the function that we are trying to learn from the data is the probability mass function over the digits conditional on the input image\n",
    "\n",
    "<img src=\"Picture_1.png\" width=\"350\" style=\"display: block; margin: 0 auto\">\n",
    "<img src=\"Picture_2.png\" width=\"350\" style=\"display: block; margin: 0 auto\">\n",
    "\n",
    "For some images, the digit is simple to identify and the model is expected to assign all probability mass to the corresponding digit class. For others, the handwriting might be more difficult to decipher and the probablity should be spread across the class labels more evenly.\n",
    "\n",
    "Formally, the probablity mass function can be denoted by\n",
    "$$\n",
    "p(y| x) \\approx f(y | x, w),\n",
    "$$\n",
    "where $y$ denotes the digit and $x$ denotes the image. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate that MLPs can, in fact, achieve much more complex tasks than the network that we have used before, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"multi_layer_perceptron_network.avif\" width=\"700\" style=\"display: block; margin: 0 auto\">\n",
    "\n",
    "While MLPs get closer to biological brains, in the sense that they use cascades of neurons to process information, it is certainly a stretch to expect MLPs to diplay intelligent behaviour. Nevertheless, a remarkable property of MLPs is that they are **universal function approximators** (arguably, the real reason why artifical \"intelligence\" has been so successful in the last decade).\n",
    "\n",
    "Universal function approximator means that MLPs can perfectly describe any function, provided: 1. we have sufficient data from the function; and 2. the MLP has sufficiently many artificial neurons in each layer. For more details, google \"Universal function approximation theorem\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edc2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

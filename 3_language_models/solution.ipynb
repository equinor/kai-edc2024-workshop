{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models\n",
    "\n",
    "In this notebook, we explore how neural networks are used for language tasks, and show how to train a model to write Shakespearian sonnets!\n",
    "In the last session, we explored the value of inductive biases in designing neural networks, so we start with a discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 From Application to Challenges: Why We Model Language\n",
    "\n",
    "## 1.1 Applications of Language Models\n",
    "Language models allow computers to understand and generate human-like text, which has various applications, such as:\n",
    "- Machine Translation: converting texts from one language to another;\n",
    "- Text Generation: generating natural language sentences or paragraphs;\n",
    "- Sentiment Analysis: determining the attitude or emotion expressed in a piece of text, e.g. in product reviews;\n",
    "- Question Answering;\n",
    "- Text Summarization;\n",
    "\n",
    "## 1.2 Challenges with Language Modelling\n",
    "However, despite the progress made in language modelling, there are still several challenges that need to be addressed:\n",
    "- Ambiguity: words or phrases with several possible meanings or interpretations;\n",
    "- Context: the meaning of a word can depend on the context it appears in;\n",
    "- Out-of-vocabulary (OOV) words: some words don't appear in the training data (e.g. new internet slang);\n",
    "- Long-term dependencies: understanding the meaning of a sentence often requires keeping track of information from earlier in the text;\n",
    "- Domain-specific knowledge: e.g. medical texts will contain a lot of specialized medical terminologies.\n",
    "\n",
    "Despite the challenges, language models are vital to enable machines to understand and generate natural language, paving the way for a wide range of NLP applications.\n",
    "\n",
    "### Task 1: In pairs, discuss how you would design a neural network architecture to be able to process language. What features would it have?  What aspects of language are you capturing with this architecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Getting the Data\n",
    "\n",
    "Our goal is to create a model that generates Shakespearian sonnets. \n",
    "One of the easiest ways to do this is to give the model some Shakespearian text and get it to predict the next letter.\n",
    "For example, if we give the model \"to be or not to b\", it can output \"e\" to complete the phrase - \"to be or not to be\".\n",
    "Then, if we give that output to the model as input, the model can give us the next character, and so on.\n",
    "We might not get \"to be or not to be, that is the question.\" as the final output, but we can get something that sounds vaguely Shakespearian.\n",
    "\n",
    "But first, we need to get the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# Download the Shakespeare dataset\n",
    "shakespeare_url = \"https://homl.info/shakespeare\"\n",
    "filepath = os.path.join(os.getcwd(), \"shakespeare.txt\")\n",
    "urllib.request.urlretrieve(shakespeare_url, filepath)\n",
    "\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()\n",
    "\n",
    "# Shows a short text sample\n",
    "print(shakespeare_text[:420])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's Shakespeare, alright! \n",
    "\n",
    "The input to our model will be a the beginning of a Shakespeare sonnet (i.e. a sequence of characters).\n",
    "Given this sequence of characters, we want our model to predict the next character.\n",
    "For simplicity, we will only use **lowercase** characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary: \n",
      " !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\n",
      "Number of distinct characters: 39\n"
     ]
    }
   ],
   "source": [
    "vocab = \"\".join(sorted(set(shakespeare_text.lower())))\n",
    "vocab_size = len(set(shakespeare_text.lower()))\n",
    "\n",
    "print(\"Our vocabulary: \" + vocab)\n",
    "print(\"Number of distinct characters: \" + str(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Creating the Training Dataset\n",
    "\n",
    "The inputs to a neural network must be numerical, so we must encode every character as an integer.\n",
    "\n",
    "It's easiest to do this using `keras.layers.TextVectorization` layer to encode this text (i.e. convert it from characters to integer IDs).\n",
    "This layer turns raw strings into an encoded representation that can be read by neural network layers.\n",
    "We set `split=\"character\"` to get character-level encoding rather than the default word-level encoding, and we use `standardize=\"lower\"` to convert the text to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Original text:\n",
      " First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "--- Encoded sequence:\n",
      " tf.Tensor(\n",
      "[21  7 10  9  4  2 20  7  4  7 37  3 11 25 12 23  3 21  5 10  3  2 18  3\n",
      "  2 24 10  5 20  3  3 14  2  6 11 17  2 21 15 10  4  8  3 10 19  2  8  3\n",
      "  6 10  2 16  3  2  9 24  3  6 26 28], shape=(60,), dtype=int64)\n",
      "\n",
      "--- Mapping of letters to integers:\n",
      " --> 0\n",
      "[UNK] --> 1\n",
      "  --> 2\n",
      "e --> 3\n",
      "t --> 4\n",
      "o --> 5\n",
      "a --> 6\n",
      "i --> 7\n",
      "h --> 8\n",
      "s --> 9\n",
      "r --> 10\n",
      "n --> 11\n",
      "\n",
      " --> 12\n",
      "l --> 13\n",
      "d --> 14\n",
      "u --> 15\n",
      "m --> 16\n",
      "y --> 17\n",
      "w --> 18\n",
      ", --> 19\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a TextVectorization layer\n",
    "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\",\n",
    "                                                   standardize=\"lower\")\n",
    "\n",
    "# Build a vocabulary of all characters in the Shakespeare text\n",
    "text_vec_layer.adapt([shakespeare_text])\n",
    "\n",
    "# Use text_vec_layer on shakespeare_text to obtain encoded character ID sequences\n",
    "encoded = text_vec_layer([shakespeare_text])[0]\n",
    "\n",
    "# Visualize the encoding\n",
    "print(\"--- Original text:\\n\", shakespeare_text[:60])\n",
    "print(\"\\n--- Encoded sequence:\\n\", encoded[:60])\n",
    "print(\"\\n--- Mapping of letters to integers:\")\n",
    "for i, char in enumerate(text_vec_layer.get_vocabulary()[:20]):\n",
    "    print(char, \"-->\", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above output, we can see that each character is now mapped to an integer, starting at 2. \n",
    "\n",
    "The `TextVectorization` layer reserved the value 0 for padding tokens, and it reserved 1 for unknown characters.\n",
    "We won’t need either of these tokens for now because neither are in the vocabulary, so we won't be using them to write our sonnets either.\n",
    "(When have you seen Shakespeare make up unknown characters? That's why.)\n",
    "\n",
    "Let’s subtract 2 from the character IDs and compute the number of distinct characters and the total number of characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  1115394\n",
      "Number of tokens:  39\n"
     ]
    }
   ],
   "source": [
    "# Drop tokens 0 (pad) and 1 (unknown) by subtracting 2 from the character IDs\n",
    "encoded -= 2\n",
    "\n",
    "# Compute the number of distinct characters\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2\n",
    "\n",
    "# Compute the total number of characters \n",
    "dataset_size = len(encoded)\n",
    "\n",
    "print(\"Dataset size: \", dataset_size)\n",
    "print(\"Number of tokens: \", n_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've said, our aim is to give the model a sequence of characters (e.g. \"to be or not to b\"), and get it to output the next letter \"e\".\n",
    "We can also frame this as the input being \"to be or not to b\" being turned into output as \"o be or not to be\" sequence and target as \"o be or not to be\" sequence.\n",
    "This target sequence indicates that for a given input sequence, the next character should be \"e\".\n",
    "\n",
    "To train such a sequence-to-sequence RNN, we can convert this long sequence into input/target pairs.\n",
    "This dataset creation involves dividing the data into windows of a fixed size. \n",
    "The model can then be trained on these input/target pairs to learn the underlying patterns in the text and generate more text of a similar style. \n",
    "\n",
    "The function `to_dataset` will convert our long sequence of character IDs (encoded text) into a dataset of input/target window pairs.\n",
    "\n",
    "### Task 2: In the code below, create input/output sequences by taking first `length` characters as input and last `length` characters as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
    "\n",
    "    # Prepare dataset of character IDs to be processed by tensorflow.\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "\n",
    "    # Create windows of size length + 1.\n",
    "    ds = ds.window(size=length + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=100_000, seed=seed)\n",
    "\n",
    "    # Batch the resulting dataset\n",
    "    ds = ds.batch(batch_size=batch_size)\n",
    "\n",
    "    # TODO: Create input/output sequences by taking first *length* characters \n",
    "    # as input and last *length* characters as output.\n",
    "    # Hint: using the map() method on ds, create a tuple with the first [length]\n",
    "    # characters as the 1st element and the last [length] characters as the 2nd element; \n",
    "    # [window] has shape (batch_size, length + 1)\n",
    "    ds = ds.map(lambda window: (window[:, :-1], window[:, 1:]))\n",
    "    \n",
    "    return ds.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This diagram illustrates what `to_dataset` is doing:\n",
    "\n",
    "<img src=\"to_dataset.png\" width=\"500\" style=\"display: block; margin: 0 auto\">\n",
    "\n",
    "Batching is a technique used to divide large datasets into smaller subsets or batches.\n",
    "Instead of feeding the entire dataset (of our input/output pairs of windows) to the model at once, we divide it into batches, which are fed to the model one-by-one during training.\n",
    "Each batch is processed independently, and the model updates its weights after processing each batch.\n",
    "Batching makes training more efficient. \n",
    "\n",
    "Let's look an an example of `to_dataset()`. \n",
    "The code below creates a dataset with a single training example: an input/output pair.\n",
    "The input represents \"to b\" and the output represents \"o be\", so the model should learn to predict the next character, i.e., \"e\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 4  5  2 23]], shape=(1, 4), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[ 4,  5,  2, 23]])>,\n",
       "  <tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[ 5,  2, 23,  3]])>)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(to_dataset(text_vec_layer([\"To be\"])[0], length=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the entire dataset is 1,115,394 characters long and we have limited time, we will use a smaller portion of the dataset to make sure we can finish training during this workshop.\n",
    "We will split it up so we use roughly 90% for training, 5% for validation and the remaining 5% for testing.\n",
    "\n",
    "We initially specified the window length as 100, but it is worth experimenting with different window lengths.\n",
    "While shorter lengths make it easier and quicker to train the RNN, as the RNN is not able to learn any pattern that is longer than the specified length, it is important to avoid choosing a window length that is too small.\n",
    "\n",
    "### Task 3: Slice the data into training, validation and test sets using the proportions specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[13 23 10 ...  8  1  4]\n",
      " [ 4  2  7 ...  3 21 11]\n",
      " [ 0  3 19 ... 10 21  1]\n",
      " ...\n",
      " [14 15  0 ...  0  4  7]\n",
      " [ 1  8  4 ...  5  7  6]\n",
      " [13  8  0 ...  1  2  0]], shape=(32, 100), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[13 10 16 ...  0  2  3]\n",
      " [ 8 15  0 ... 25  5  7]\n",
      " [ 9  1 17 ... 14  0  5]\n",
      " ...\n",
      " [ 2  5 35 ...  0 14  1]\n",
      " [ 8 23 10 ...  2 15  0]\n",
      " [ 3  8  7 ...  2  6  8]], shape=(32, 100), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[11  3 25 ... 11  4 18]\n",
      " [ 3 25  1 ...  4 18  1]\n",
      " [25  1  7 ... 18  1 26]\n",
      " ...\n",
      " [ 0  9  3 ...  8  5  3]\n",
      " [ 9  3  2 ...  5  3 11]\n",
      " [ 3  2  0 ...  3 11  4]], shape=(32, 100), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[ 2  0  2 ... 11  4  9]\n",
      " [ 0  2  3 ...  4  9 13]\n",
      " [ 2  3  0 ...  9 13  7]\n",
      " ...\n",
      " [ 6 15  0 ...  4  2  0]\n",
      " [15  0 18 ...  2  0 15]\n",
      " [ 0 18  3 ...  0 15  3]], shape=(32, 100), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[27 12  0 ...  2  6  4]\n",
      " [12  0  4 ...  6  4  9]\n",
      " [ 0  4  2 ...  4  9  0]\n",
      " ...\n",
      " [ 0 22  8 ... 28  0  8]\n",
      " [22  8  1 ...  0  8  1]\n",
      " [ 8  1 18 ...  8  1 16]], shape=(32, 100), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[ 1 18  5 ...  1 16  4]\n",
      " [18  5  3 ... 16  4  8]\n",
      " [ 5  3 13 ...  4  8 12]\n",
      " ...\n",
      " [ 9  0 14 ...  6  1 14]\n",
      " [ 0 14 13 ...  1 14 17]\n",
      " [14 13 18 ... 14 17  0]], shape=(32, 100), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "length = 100\n",
    "subset_proportion = 0.05\n",
    "reduced_dataset_size = int(dataset_size * subset_proportion)\n",
    "\n",
    "# TODO: Slice the encoded data into training, validation, and test sets using \n",
    "# the proportions 90%, 5%, 5%\n",
    "train_encoded = encoded[:int(reduced_dataset_size * 0.9)]\n",
    "validation_encoded = encoded[int(reduced_dataset_size * 0.9):int(reduced_dataset_size * 0.95)]\n",
    "test_encoded = encoded[int(reduced_dataset_size * 0.95):reduced_dataset_size]\n",
    "\n",
    "# Create datasets\n",
    "tf.random.set_seed(42)\n",
    "train_set = to_dataset(train_encoded, length=length, shuffle=True, seed=42)\n",
    "valid_set = to_dataset(validation_encoded, length=length)\n",
    "test_set = to_dataset(test_encoded, length=length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Training Our Own Shakespeare\n",
    "\n",
    "Since our dataset is reasonably large, and modeling language is quite a difficult task, we need more than a simple RNN with a few recurrent neurons.\n",
    "Let’s build and train a model with one GRU layer (type of RNN layer) composed of 128 units (you can try tweaking the number of layers and units later, if needed).\n",
    "\n",
    "Let’s go over this code:\n",
    "\n",
    "- We use an `Embedding` layer as the first layer, to encode the character IDs (embeddings were introduced in Chapter 13). The `Embedding` layer’s number of input dimensions is the number of distinct character IDs, and the number of output dimensions is a hyperparameter you can tune — we’ll set it to 16 for now. Whereas the inputs of the `Embedding` layer will be 2D tensors of shape *[batch size, window length]*, the output of the Embedding layer will be a 3D tensor of shape *[batch size, window length, embedding size]*.\n",
    "\n",
    "- We use a `Dense` layer for the output layer: it must have 39 units (n_tokens) because there are 39 distinct characters in the text, and we want to output a probability for each possible character (at each time step). The 39 output probabilities should sum up to 1 at each time step, so we apply the softmax activation function to the outputs of the Dense layer.\n",
    "\n",
    "- Lastly, we compile this model, using the `\"sparse_categorical_crossentropy\"` loss and a Nadam optimizer, and we train the model for several epochs, using a `ModelCheckpoint` callback to save the best model (in terms of validation accuracy) as training progresses.\n",
    "\n",
    "### Task 4: Experiment with different values for epochs\n",
    "In machine learning, an epoch refers to one iteration of the entire training dataset through a neural network, i.e. one pass forward and backward through the model.\n",
    "During training, the data is usually divided into batches, and the training process involves iterating through all the batches in one complete iteration.\n",
    "Increasing the number of epochs will increase the number of times the model gets to refine its weights and improve its predictions on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "   1566/Unknown - 81s 49ms/step - loss: 2.0356 - accuracy: 0.4062INFO:tensorflow:Assets written to: my_shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566/1566 [==============================] - 84s 50ms/step - loss: 2.0356 - accuracy: 0.4062 - val_loss: 1.8650 - val_accuracy: 0.4454\n",
      "Epoch 2/3\n",
      "1566/1566 [==============================] - 85s 52ms/step - loss: 1.2603 - accuracy: 0.6178 - val_loss: 2.1912 - val_accuracy: 0.4187\n",
      "Epoch 3/3\n",
      "1566/1566 [==============================] - 85s 51ms/step - loss: 0.9146 - accuracy: 0.7294 - val_loss: 2.5979 - val_accuracy: 0.3976\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True),\n",
    "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compile the model, i.e. give it loss function, optimizer and metrics\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# A callback is a set of functions that can be applied during training to \n",
    "# perform various tasks, such as saving the best model weights, early stopping \n",
    "# if the validation loss stops improving, etc.\n",
    "# Create a ModelCheckpoint callback that saves the best model weights to a file\n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"my_shakespeare_model\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True)\n",
    "\n",
    "# Train the model using the fit() method. Pass the training and validation sets \n",
    "# to the train_set and valid_set parameters, respectively. \n",
    "# TODO: Choose the number of epochs to train the model for (e.g. 2-5)\n",
    "history = model.fit(train_set,\n",
    "                    validation_data=valid_set,\n",
    "                    epochs=3,\n",
    "                    callbacks=[model_ckpt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model does not handle text preprocessing, so let’s wrap it in a final model containing the `tf.keras.layers.TextVectorization` layer as the first layer, plus a `tf.keras.layers.Lambda` layer to subtract 2 from the character IDs (since we’re not using the padding and unknown tokens for now):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add text preprocessing to the model\n",
    "model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Lambda(lambda X: X - 2),  # no <PAD> or <UNK> tokens\n",
    "    model\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since model training takes a long time, we have a pretrained model for you.\n",
    "The following code will download it.\n",
    "Uncomment the last line if you want to use it instead of the model trained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Downloads a pretrained model\n",
    "url = \"https://github.com/ageron/data/raw/main/shakespeare_model.tgz\"\n",
    "path = tf.keras.utils.get_file(\"shakespeare_model.tgz\", url, extract=True) \n",
    "model_path = Path(path).with_name(\"shakespeare_model\")\n",
    "# model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it a spin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 206ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the predict method on [\"To be or not to b\"]\n",
    "# original array is nested, so need to access the first element, \n",
    "# get the last element of the array, i.e. the last letter, the prediction\n",
    "y_prob = model.predict([\"To be or not to b\"])[0, -1] \n",
    "y_pred = tf.argmax(y_prob)  # choose the most probable character ID\n",
    "\n",
    "# Use the vocabulary of the text_vec_layer to get the character corresponding to y_pred\n",
    "text_vec_layer.get_vocabulary()[y_pred + 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! Our model made a prediction (hopefully a correct one)!\n",
    "It is now ready to write full sonnets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Making Inferences, i.e. writing sonnets\n",
    "\n",
    "To generate new text using the char-RNN model, we could feed it some text, make the model predict the most likely next letter, add it to the end of the text, then give the extended text to the model to guess the next letter, and so on.\n",
    "This is called greedy decoding.\n",
    "But in practice this often leads to the same words being repeated over and over again.\n",
    "\n",
    "Instead, we can sample the next character randomly, with a probability equal to the estimated probability, using TensorFlow’s `tf.random.categorical()` function.\n",
    "This will generate more diverse and interesting text. The `categorical()` function samples random class indices, given the class log probabilities (logits). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 8), dtype=int64, numpy=array([[0, 1, 0, 2, 1, 0, 0, 1]])>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probas = tf.math.log([[0.5, 0.4, 0.1]])  # probas = 50%, 40%, and 10%\n",
    "tf.random.categorical(log_probas, num_samples=8)  # draw 8 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have more control over the diversity of the generated text, we can divide the logits by a number called the **temperature**, which we can tweak as we wish.\n",
    "A temperature close to zero favors high-probability characters, while a high temperature gives all characters an equal probability.\n",
    "Lower temperatures are typically preferred when generating fairly rigid and precise text, such as mathematical equations, while higher temperatures are preferred when generating more diverse and creative text.\n",
    "\n",
    "The following `next_char()` helper function uses this approach to pick the next character to add to the input text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "\n",
    "    # Generate the predicted probabilities for the next character in the \n",
    "    # sequence based on the current text\n",
    "    # Select the final output vector from this prediction, \n",
    "    # i.e. the last character in the sequence\n",
    "    y_proba = model.predict([text])[0, -1:]\n",
    "    \n",
    "    # Rescale the probability distribution using the temperature parameter\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "\n",
    "    # Sample the next character ID from this rescaled distribution\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
    "\n",
    "    # Return the character corresponding to the sampled ID\n",
    "    return text_vec_layer.get_vocabulary()[char_id + 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can write another small helper function that will repeatedly call `next_char()` to get the next character and append it to the given text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all we need!\n",
    "\n",
    "### Task 5: Tune the temperature to see the impact on sonnet quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "To be or not to bear at the people in many to him, and the hather be\n"
     ]
    }
   ],
   "source": [
    "# TODO: change the temperature parameter to see the effect on the generated text\n",
    "print(extend_text(\"To be or not to be\", temperature=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 More on Language Models\n",
    "\n",
    "In this notebook, we explored the essential concepts of Language Models and the various applications they have in the field of NLP.\n",
    "We also discussed the challenges that come with building an accurate language model, such as ambiguity, context, out-of-vocabulary words, long-term dependencies and data sparsity.\n",
    "\n",
    "While we were able to build a simple language model that works at the character level, we must keep in mind that natural language is much more complex than this. \n",
    "Language models that can also understand the structure of words in sentences and comprehend their meaning require more sophisticated architectures and techniques such as Word Embeddings, Recurrent Neural Networks and Transformers.\n",
    "\n",
    "We encourage you to take what you have learned in this workshop and experiment with more with the Shakespeare model: increase the proportion of the data used, train for more epochs and add more layers. \n",
    "(The pre-trained model we loaded used 10 epochs and the full training set).\n",
    "Additionally, you can also experiment with different text preprocessing techniques, different architectures and hyperparameters to achieve better results. \n",
    "The possibilities are endless, and there is always more to learn!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edc2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

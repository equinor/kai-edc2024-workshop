{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models\n",
    "\n",
    "In this notebook, we build on the concepts that we have learned in the first two sessions of the workshop and develop neural networks for language processing. \n",
    "\n",
    "**Learning outcomes:**\n",
    "1. A conceptual understanding how modern neural networks process language;\n",
    "2. The use of inductive biases for learning generalisable concepts of language; and\n",
    "3. An understanding of the limitations of current neural network-based language modelling approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import kai\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Inductive biases for language\n",
    "\n",
    "In the previous sessions, we saw that neural networks are incredibly versatile and can be used to solve many complex tasks. We specifically focussed on MLPs and their ability to approximate any function using sufficiently many input-output data points (universal function approximation theorem). However, we also highlighted that MLPs are not good at learning concepts that they can use to generalise beyond the training data. This left us with a mediocre Q&A neural network which was limited to answering a pre-defined set of questions. To improve the generalisability of neural networks, we discussed inductive biases and the role of prior knowledge to enable more efficient training. In this session, we can finally begin to model language!\n",
    "\n",
    "One of the challenges with our Q&A neural network was that our model didn't really have a sense of language -- we simply mapped question IDs to answer IDs. So, perhaps, one way to approach Q&A is to take a step back and to just focus on the language component of the task for the moment. In other words, we remove the requirement that the neural network needs to retain knowledge and are simply left with building a model that can produce grammatically correct sentences. Such a neural network is more useful than you might think and, in fact, most of us have probably a device with such a neural network in our pockets right now -- the text auto-completion functionality of the keyboards in our phones!\n",
    "\n",
    "### Task 1: Discuss which inductive biases could be useful for text auto-completion?\n",
    "\n",
    "Inductive biases, such as translational invariance, seem quite intuitive for image classification. But what are useful inductive biases for language?\n",
    "\n",
    "*Hint 1:* Remember that last time, we had a problem to generalise our Q&A MLP model because we enumerated complete questions and complete answers. This time, think, perhaps, a little bit more granular. What if we enumerate words, or even letters. Is there an inductive <*blank*> that we can use to help with the language processing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can document your thoughts here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how many preceeding words do we need to make good predictions? For the \"inductive <*blank*>\" example, one preceding word is sufficient to be pretty confident that the next word is \"bias\". But what about other <*blank*>? Now it is less clear what the missing word is! It could be \"words\", it could be \"examples\", or it could be \"instances\", or it could be something completely different. This shows that by only looking at one preceeding word there is a certain level of ambuiguity in language.\n",
    "\n",
    "This ambiguity can be resolved (to some degree) by looking at more than just one word. For example by looking at the entire preceeding sentence \"For the \"inductive <*blank*>\" example, one preceding word is sufficient to be pretty confident that the next word is \"bias\".\", it may be more obvious that the most likely word is \"words\", while \"examples\" might also work for the sentence. Other words seem less likely.\n",
    "\n",
    "This illustrates that text auto-completion is context-dependent. The information provided by the previous words influences the likelihood of the next word! To get a feeling for how many words are needed to make relatively confident next word predictions, let us analyse an example text: Shakespeare's A Midsummer Night's Dream, and let us, for simplicity, focus on letters intead of on words. We can determine the level of ambiguity of the next-letter prediction by counting how many different letters occur following a given sequence of letters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Empirically determine a useful context-size\n",
    "\n",
    "Below is the dataset which contains the text from Shakespeare's A Midsummer Night's Dream. Count how many different letters follow the following (arbitrarily chosen) sequences:\n",
    "\n",
    "1. l\n",
    "2. lov\n",
    "3. love ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise dataset\n",
    "text = kai.get_midsummer_night_dream()\n",
    "\n",
    "# Shows a short text sample\n",
    "print(text[:420])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find how many different letters follow the below patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise count\n",
    "kai.plot_letter_count(bin1, bin2, bin3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! We can see that the more letters we keep track of, the less ambiguity there is with regard to the next letter, and by keeping track of 5 letters (including the space symbol as a letter), we are down to three options for the sequence \"love c\". This suggests that a relatively short context size of around 5 letters could perhaps be enough to make quite confident next letter predictions!\n",
    "\n",
    "Note that if we can predict the next letter, we can also predict next words! By simply including predictions as context for the next prediction. This is illustrated in the figure below for next word prediction (because we couldn't find a similar gif for letter prediction).\n",
    "\n",
    "<img src=\"next-word-pred.gif\" width=\"700\" style=\"display: block; margin: 0 auto\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Define a vocabulary for your language model\n",
    "\n",
    "Now it's time to implement our first language model! The first component of any language model is to define the inputs and outputs. So we need to transate words or letters into numerical values. These numerical values are typically referred to as *tokens*. Here, we want to keep it simple and therefore just enumerate all unique characters that appear in Shakespeare's text. \n",
    "\n",
    "*Aside:* Modern language model's use more complex \"tokenisers\" which take more prior knowledge about a language into account (for example it can make sentence to have one token for the stem of a word and several tokens for the different conjugations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Execute the below cell to define the vocabulary for our language model\n",
    "class Vocabulary():\n",
    "    def __init__(self, text):\n",
    "        # TODO:\n",
    "\n",
    "    def get_letters(self, indices):\n",
    "        # TODO: Returns letters corresponding to indices\n",
    "        # Input: list of integers\n",
    "        # Output: list of tokens (str)\n",
    "\n",
    "    def get_indices(self, letters):\n",
    "        # TODO: Returns indices corresponding to letters\n",
    "        # Input: list of integers\n",
    "        # Output: list of tokens (str)\n",
    "\n",
    "# Example\n",
    "vocab = Vocabulary(text=text)\n",
    "print(f'Letters: {text[:20]}')\n",
    "print(f'Indices: {vocab.get_indices(text[:20])}')\n",
    "print(f'Letters: {vocab.get_letters(vocab.get_indices(text[:20]))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Define a MLP that takes in 5 tokens (letter IDs) and predicts the probability masses over the vocabulary\n",
    "\n",
    "The 10 input tokens is the window size of the language model and the output probability masses are the predictions of the model for the next letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, context_size=5, output_size=48):\n",
    "        super(MLP, self).__init__()\n",
    "        # TODO:\n",
    "\n",
    "    def forward(self,x):\n",
    "        # TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Train the language model on A Midsummers Night's Dream\n",
    "\n",
    "For simplicity, we have already prepared a DataLoader that splits Shakespeare's play into a training and test set and also parses the text into batches of 10 tokens with subsequent 11th token that needs to be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "context_size = 5\n",
    "trainloader, testloader = kai.get_shakespeare_dataloader(\n",
    "    window_size=context_size, batch_size=128)\n",
    "\n",
    "# Train Network\n",
    "learning_rate = 0.0005\n",
    "num_epochs = 50\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MLP(context_size=context_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "accuracy = []\n",
    "accuracy.append(kai.check_accuracy_language(trainloader, model, device))\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch: {epoch}')\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(trainloader):\n",
    "\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()  # set all gradients to zero for each batch\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent\n",
    "        optimizer.step()\n",
    "        losses.append(loss.detach().numpy())\n",
    "\n",
    "    accuracy.append(kai.check_accuracy_language(trainloader, model, device))\n",
    "\n",
    "kai.plot_loss(losses)\n",
    "kai.plot_accuracy(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the accuracy of our language model is not great but also not too bad given that we only consider 5 preceeding letters and we didn't train the model for very long. Let us see how well the model performs when we try to complete a word!\n",
    "\n",
    "### Task 6: Auto-complete the below text examples\n",
    "\n",
    "predict next letters until a space (``' '``), tab (``'\\t'``) or new line (``'\\n'``) is predicted. Use the above defined vocabulary to convert between letters and tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "def auto_complete(text, model):\n",
    "    # Stop tokens\n",
    "    stop_tokens = vocab.get_indices([' ', '\\t', '\\n'])\n",
    "\n",
    "    # TODO: Complete implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Auto-complete the words\n",
    "# 1\n",
    "test = 'love c'\n",
    "print('Start: ', test)\n",
    "prediction = auto_complete(test, model)\n",
    "prediction = vocab.get_letters(prediction)\n",
    "print('Auto-complete: ', ''.join(prediction))\n",
    "\n",
    "# 2\n",
    "test = 'digni'\n",
    "print('Start: ', test)\n",
    "prediction = auto_complete(test, model)\n",
    "prediction = vocab.get_letters(prediction)\n",
    "print('Auto-complete: ', ''.join(prediction))\n",
    "\n",
    "# 3\n",
    "test = 'waggi'\n",
    "print('Start: ', test)\n",
    "prediction = auto_complete(test, model)\n",
    "prediction = vocab.get_letters(prediction)\n",
    "print('Auto-complete: ', ''.join(prediction))\n",
    "\n",
    "# 4\n",
    "test = 'intelligenc'\n",
    "print('Start: ', test)\n",
    "prediction = auto_complete(test, model)\n",
    "prediction = vocab.get_letters(prediction)\n",
    "print('Auto-complete: ', ''.join(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok... this is not working at all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Discuss why the accuracy of model is relatively high but the predictions make no sense?\n",
    "\n",
    "*Hint:* The bad predictions are not related to model architecture or the number of training epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document your thoughts here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Use a sequence of predictions to improved the performance of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "context_size = 5\n",
    "n_sequential = 5\n",
    "trainloader2, _ = kai.get_shakespeare_dataloader(\n",
    "    window_size=context_size+n_sequential, batch_size=128)\n",
    "\n",
    "# Train Network\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 100\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MLP(context_size=context_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "accuracy = []\n",
    "accuracy.append(kai.check_accuracy_language(trainloader, model, device))\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch: {epoch}')\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(trainloader2):\n",
    "\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # TODO: Amend this to iteratively predict the next tokens\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        #### End\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()  # set all gradients to zero for each batch\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent\n",
    "        optimizer.step()\n",
    "        losses.append(loss.detach().numpy())\n",
    "\n",
    "    accuracy.append(kai.check_accuracy_language(trainloader, model, device))\n",
    "\n",
    "kai.plot_loss(losses)\n",
    "kai.plot_accuracy(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Auto-complete the words\n",
    "# 1\n",
    "test = 'love c'\n",
    "print('Start: ', test)\n",
    "prediction = auto_complete(test, model)\n",
    "prediction = vocab.get_letters(prediction)\n",
    "print('Auto-complete: ', ''.join(prediction))\n",
    "\n",
    "# 2\n",
    "test = 'digni'\n",
    "print('Start: ', test)\n",
    "prediction = auto_complete(test, model)\n",
    "prediction = vocab.get_letters(prediction)\n",
    "print('Auto-complete: ', ''.join(prediction))\n",
    "\n",
    "# 3\n",
    "test = 'waggi'\n",
    "print('Start: ', test)\n",
    "prediction = auto_complete(test, model)\n",
    "prediction = vocab.get_letters(prediction)\n",
    "print('Auto-complete: ', ''.join(prediction))\n",
    "\n",
    "# 4\n",
    "test = 'intelligenc'\n",
    "print('Start: ', test)\n",
    "prediction = auto_complete(test, model)\n",
    "prediction = vocab.get_letters(prediction)\n",
    "print('Auto-complete: ', ''.join(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 A more advanced language model\n",
    "\n",
    "Our goal is to create a model that generates Shakespearian sonnets. \n",
    "One of the easiest ways to do this is to give the model some Shakespearian text and get it to predict the next letter.\n",
    "For example, if we give the model \"to be or not to b\", it can output \"e\" to complete the phrase - \"to be or not to be\".\n",
    "Then, if we give that output to the model as input, the model can give us the next character, and so on.\n",
    "We might not get \"to be or not to be, that is the question.\" as the final output, but we can get something that sounds vaguely Shakespearian.\n",
    "\n",
    "But first, we need to get the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Download the Shakespeare dataset\n",
    "shakespeare_url = \"https://homl.info/shakespeare\"\n",
    "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()\n",
    "\n",
    "# Shows a short text sample\n",
    "print(shakespeare_text[:420])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's Shakespeare, alright! \n",
    "\n",
    "The input to our model will be a the beginning of a Shakespeare sonnet (i.e. a sequence of characters).\n",
    "Given this sequence of characters, we want our model to predict the next character.\n",
    "For simplicity, we will only use **lowercase** characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = \"\".join(sorted(set(shakespeare_text.lower())))\n",
    "vocab_size = len(set(shakespeare_text.lower()))\n",
    "\n",
    "print(\"Our vocabulary: \" + vocab)\n",
    "print(\"Number of distinct characters: \" + str(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Creating the Training Dataset\n",
    "\n",
    "The inputs to a neural network must be numerical, so we must encode every character as an integer.\n",
    "\n",
    "It's easiest to do this using `keras.layers.TextVectorization` layer to encode this text (i.e. convert it from characters to integer IDs).\n",
    "This layer turns raw strings into an encoded representation that can be read by neural network layers.\n",
    "We set `split=\"character\"` to get character-level encoding rather than the default word-level encoding, and we use `standardize=\"lower\"` to convert the text to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TextVectorization layer\n",
    "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\",\n",
    "                                                   standardize=\"lower\")\n",
    "\n",
    "# Build a vocabulary of all characters in the Shakespeare text\n",
    "text_vec_layer.adapt([shakespeare_text])\n",
    "\n",
    "# Use text_vec_layer on shakespeare_text to obtain encoded character ID sequences\n",
    "encoded = text_vec_layer([shakespeare_text])[0]\n",
    "\n",
    "# Visualize the encoding\n",
    "print(\"--- Original text:\\n\", shakespeare_text[:60])\n",
    "print(\"\\n--- Encoded sequence:\\n\", encoded[:60])\n",
    "print(\"\\n--- Mapping of letters to integers:\")\n",
    "for i, char in enumerate(text_vec_layer.get_vocabulary()[:20]):\n",
    "    print(char, \"-->\", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above output, we can see that each character is now mapped to an integer, starting at 2. \n",
    "\n",
    "The `TextVectorization` layer reserved the value 0 for padding tokens, and it reserved 1 for unknown characters.\n",
    "We won’t need either of these tokens for now because neither are in the vocabulary, so we won't be using them to write our sonnets either.\n",
    "(When have you seen Shakespeare make up unknown characters? That's why.)\n",
    "\n",
    "Let’s subtract 2 from the character IDs and compute the number of distinct characters and the total number of characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop tokens 0 (pad) and 1 (unknown) by subtracting 2 from the character IDs\n",
    "encoded -= 2\n",
    "\n",
    "# Compute the number of distinct characters\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2\n",
    "\n",
    "# Compute the total number of characters\n",
    "dataset_size = len(encoded)\n",
    "\n",
    "print(\"Dataset size: \", dataset_size)\n",
    "print(\"Number of tokens: \", n_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've said, our aim is to give the model a sequence of characters (e.g. \"to be or not to b\"), and get it to output the next letter \"e\".\n",
    "We can also frame this as the input being \"to be or not to b\" being turned into output as \"o be or not to be\" sequence and target as \"o be or not to be\" sequence.\n",
    "This target sequence indicates that for a given input sequence, the next character should be \"e\".\n",
    "\n",
    "To train such a sequence-to-sequence RNN, we can convert this long sequence into input/target pairs.\n",
    "This dataset creation involves dividing the data into windows of a fixed size. \n",
    "The model can then be trained on these input/target pairs to learn the underlying patterns in the text and generate more text of a similar style. \n",
    "\n",
    "The function `to_dataset` will convert our long sequence of character IDs (encoded text) into a dataset of input/target window pairs.\n",
    "\n",
    "### Task 2: In the code below, create input/output sequences by taking first `length` characters as input and last `length` characters as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
    "\n",
    "    # Prepare dataset of character IDs to be processed by tensorflow.\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "\n",
    "    # Create windows of size length + 1.\n",
    "    ds = ds.window(size=length + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=100_000, seed=seed)\n",
    "\n",
    "    # Batch the resulting dataset\n",
    "    ds = ds.batch(batch_size=batch_size)\n",
    "\n",
    "    # TODO: Create input/output sequences by taking first *length* characters\n",
    "    # as input and last *length* characters as output.\n",
    "    # Hint: using the map() method on ds, and use the lambda function to\n",
    "    # create a tuple with the first length characters as the first element\n",
    "    # and the last length characters as the second element\n",
    "    ds = ds.map(lambda window: (window[:, :-1], window[:, 1:]))\n",
    "\n",
    "    return ds.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This diagram illustrates what `to_dataset` is doing:\n",
    "\n",
    "<img src=\"to_dataset.png\" width=\"500\" style=\"display: block; margin: 0 auto\">\n",
    "\n",
    "Batching is a technique used to divide large datasets into smaller subsets or batches.\n",
    "Instead of feeding the entire dataset (of our input/output pairs of windows) to the model at once, we divide it into batches, which are fed to the model one-by-one during training.\n",
    "Each batch is processed independently, and the model updates its weights after processing each batch.\n",
    "Batching makes training more efficient. \n",
    "\n",
    "Let's look an an example of `to_dataset()`. \n",
    "The code below creates a dataset with a single training example: an input/output pair.\n",
    "The input represents \"to b\" and the output represents \"o be\", so the model should learn to predict the next character, i.e., \"e\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(to_dataset(text_vec_layer([\"To be\"])[0], length=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the entire dataset is 1,115,394 characters long and we have limited time, we will use a smaller portion of the dataset to make sure we can finish training during this workshop.\n",
    "We will split it up so we use roughly 90% for training, 5% for validation and the remaining 5% for testing.\n",
    "\n",
    "We initially specified the window length as 100, but it is worth experimenting with different window lengths.\n",
    "While shorter lengths make it easier and quicker to train the RNN, as the RNN is not able to learn any pattern that is longer than the specified length, it is important to avoid choosing a window length that is too small.\n",
    "\n",
    "### Task 3: Slice the data into training, validation and test sets using the proportions specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 100\n",
    "subset_proportion = 0.05\n",
    "reduced_dataset_size = int(dataset_size * subset_proportion)\n",
    "\n",
    "# TODO: Slice the data into training, validation, and test sets using the\n",
    "# proportions 90%, 5%, 5%\n",
    "train_encoded = encoded[:int(reduced_dataset_size * 0.9)]\n",
    "validation_encoded = encoded[int(reduced_dataset_size * 0.9):int(reduced_dataset_size * 0.95)]\n",
    "test_encoded = encoded[int(reduced_dataset_size * 0.95):reduced_dataset_size]\n",
    "\n",
    "# Create datasets\n",
    "tf.random.set_seed(42)\n",
    "train_set = to_dataset(train_encoded, length=length, shuffle=True, seed=42)\n",
    "valid_set = to_dataset(validation_encoded, length=length)\n",
    "test_set = to_dataset(test_encoded, length=length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Training Our Own Shakespeare\n",
    "\n",
    "Since our dataset is reasonably large, and modeling language is quite a difficult task, we need more than a simple RNN with a few recurrent neurons.\n",
    "Let’s build and train a model with one GRU layer (type of RNN layer) composed of 128 units (you can try tweaking the number of layers and units later, if needed).\n",
    "\n",
    "Let’s go over this code:\n",
    "\n",
    "- We use an `Embedding` layer as the first layer, to encode the character IDs (embeddings were introduced in Chapter 13). The `Embedding` layer’s number of input dimensions is the number of distinct character IDs, and the number of output dimensions is a hyperparameter you can tune — we’ll set it to 16 for now. Whereas the inputs of the `Embedding` layer will be 2D tensors of shape *[batch size, window length]*, the output of the Embedding layer will be a 3D tensor of shape *[batch size, window length, embedding size]*.\n",
    "\n",
    "- We use a `Dense` layer for the output layer: it must have 39 units (n_tokens) because there are 39 distinct characters in the text, and we want to output a probability for each possible character (at each time step). The 39 output probabilities should sum up to 1 at each time step, so we apply the softmax activation function to the outputs of the Dense layer.\n",
    "\n",
    "- Lastly, we compile this model, using the `\"sparse_categorical_crossentropy\"` loss and a Nadam optimizer, and we train the model for several epochs, using a `ModelCheckpoint` callback to save the best model (in terms of validation accuracy) as training progresses.\n",
    "\n",
    "### Task 4: Experiment with different values for epochs\n",
    "In machine learning, an epoch refers to one iteration of the entire training dataset through a neural network, i.e. one pass forward and backward through the model.\n",
    "During training, the data is usually divided into batches, and the training process involves iterating through all the batches in one complete iteration.\n",
    "Increasing the number of epochs will increase the number of times the model gets to refine its weights and improve its predictions on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True),\n",
    "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compile the model, i.e. give it loss function, optimizer and metrics\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# A callback is a set of functions that can be applied during training to\n",
    "# perform various tasks, such as saving the best model weights, early stopping\n",
    "# if the validation loss stops improving, etc.\n",
    "# Create a ModelCheckpoint callback that saves the best model weights to a file\n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"my_shakespeare_model\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True)\n",
    "\n",
    "# Train the model using the fit() method. Pass the training and validation sets\n",
    "# to the train_set and valid_set parameters, respectively.\n",
    "# TODO: Choose the number of epochs to train the model for (e.g. 2-5)\n",
    "history = model.fit(train_set,\n",
    "                    validation_data=valid_set,\n",
    "                    epochs=3,\n",
    "                    callbacks=[model_ckpt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model does not handle text preprocessing, so let’s wrap it in a final model containing the `tf.keras.layers.TextVectorization` layer as the first layer, plus a `tf.keras.layers.Lambda` layer to subtract 2 from the character IDs (since we’re not using the padding and unknown tokens for now):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add text preprocessing to the model\n",
    "model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Lambda(lambda X: X - 2),  # no <PAD> or <UNK> tokens\n",
    "    model\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since model training takes a long time, we have a pretrained model for you.\n",
    "The following code will download it.\n",
    "Uncomment the last line if you want to use it instead of the model trained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Downloads a pretrained model\n",
    "url = \"https://github.com/ageron/data/raw/main/shakespeare_model.tgz\"\n",
    "path = tf.keras.utils.get_file(\"shakespeare_model.tgz\", url, extract=True)\n",
    "model_path = Path(path).with_name(\"shakespeare_model\")\n",
    "# model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it a spin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the predict method on [\"To be or not to b\"]\n",
    "# original array is nested, so need to access the first element,\n",
    "# get the last element of the array, i.e. the last letter, the prediction\n",
    "y_prob = model.predict([\"To be or not to b\"])[0, -1]\n",
    "y_pred = tf.argmax(y_prob)  # choose the most probable character ID\n",
    "\n",
    "# Use the vocabulary of the text_vec_layer to get the character corresponding to y_pred\n",
    "text_vec_layer.get_vocabulary()[y_pred + 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! Our model made a prediction (hopefully a correct one)!\n",
    "It is now ready to write full sonnets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Making Inferences, i.e. writing sonnets\n",
    "\n",
    "To generate new text using the char-RNN model, we could feed it some text, make the model predict the most likely next letter, add it to the end of the text, then give the extended text to the model to guess the next letter, and so on.\n",
    "This is called greedy decoding.\n",
    "But in practice this often leads to the same words being repeated over and over again.\n",
    "\n",
    "Instead, we can sample the next character randomly, with a probability equal to the estimated probability, using TensorFlow’s `tf.random.categorical()` function.\n",
    "This will generate more diverse and interesting text. The `categorical()` function samples random class indices, given the class log probabilities (logits). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probas = tf.math.log([[0.5, 0.4, 0.1]])  # probas = 50%, 40%, and 10%\n",
    "tf.random.categorical(log_probas, num_samples=8)  # draw 8 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have more control over the diversity of the generated text, we can divide the logits by a number called the **temperature**, which we can tweak as we wish.\n",
    "A temperature close to zero favors high-probability characters, while a high temperature gives all characters an equal probability.\n",
    "Lower temperatures are typically preferred when generating fairly rigid and precise text, such as mathematical equations, while higher temperatures are preferred when generating more diverse and creative text.\n",
    "\n",
    "The following `next_char()` helper function uses this approach to pick the next character to add to the input text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "\n",
    "    # Generate the predicted probabilities for the next character in the\n",
    "    # sequence based on the current text\n",
    "    # Select the final output vector from this prediction,\n",
    "    # i.e. the last character in the sequence\n",
    "    y_proba = model.predict([text])[0, -1:]\n",
    "\n",
    "    # Rescale the probability distribution using the temperature parameter\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "\n",
    "    # Sample the next character ID from this rescaled distribution\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
    "\n",
    "    # Return the character corresponding to the sampled ID\n",
    "    return text_vec_layer.get_vocabulary()[char_id + 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can write another small helper function that will repeatedly call `next_char()` to get the next character and append it to the given text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all we need!\n",
    "\n",
    "### Task 5: Tune the temperature to see the impact on sonnet quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extend_text(\"To be or not to be\", temperature=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 More on Language Models\n",
    "\n",
    "In this notebook, we explored the essential concepts of Language Models and the various applications they have in the field of NLP.\n",
    "We also discussed the challenges that come with building an accurate language model, such as ambiguity, context, out-of-vocabulary words, long-term dependencies and data sparsity.\n",
    "\n",
    "While we were able to build a simple language model that works at the character level, we must keep in mind that natural language is much more complex than this. \n",
    "Language models that can also understand the structure of words in sentences and comprehend their meaning require more sophisticated architectures and techniques such as Word Embeddings, Recurrent Neural Networks and Transformers.\n",
    "\n",
    "We encourage you to take what you have learned in this workshop and experiment with more with the Shakespeare model: increase the proportion of the data used, train for more epochs and add more layers. \n",
    "(The pre-trained model we loaded used 10 epochs and the full training set).\n",
    "Additionally, you can also experiment with different text preprocessing techniques, different architectures and hyperparameters to achieve better results. \n",
    "The possibilities are endless, and there is always more to learn!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edc2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

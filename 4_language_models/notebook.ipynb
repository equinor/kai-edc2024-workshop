{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Language Models\n",
    "\n",
    "A common approach for natural language tasks is to use neural networks. \n",
    "We will therefore continue to explore RNNs introduced in Section 3, training a **character RNN**, trained to predict the next character in a sentence.\n",
    "\n",
    "We follow the Char-RNN project by Andrej Karpathy (https://github.com/karpathy/char-rnn).\n",
    "We will be using his Shakespeare data to create our own Char-RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Creating the Training Dataset\n",
    "\n",
    "Let's download the Shakespeare dataset and take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Download the Shakespeare dataset\n",
    "shakespeare_url = \"https://homl.info/shakespeare\"  # shortcut URL\n",
    "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()\n",
    "\n",
    "# Shows a short text sample\n",
    "print(shakespeare_text[:420])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# Download the Shakespeare dataset\n",
    "shakespeare_url = 'https://homl.info/shakespeare'\n",
    "filepath = 'shakespeare.txt'\n",
    "\n",
    "if not os.path.exists(filepath):\n",
    "    urllib.request.urlretrieve(shakespeare_url, filepath)\n",
    "\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()\n",
    "\n",
    "# Shows a short text sample\n",
    "print(shakespeare_text[:420])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to our model will be a the beginning of a Shakespeare sonnet (i.e. a sequence of characters).\n",
    "Given this sequence of characters, we want our model to predict the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary: \n",
      " !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\n",
      "Number of distinct characters: 39\n"
     ]
    }
   ],
   "source": [
    "vocab = \"\".join(sorted(set(shakespeare_text.lower())))\n",
    "vocab_size = len(set(shakespeare_text.lower()))\n",
    "print(\"Our vocabulary: \" + vocab)\n",
    "print(\"Number of distinct characters: \" + str(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must encode every character as an integer. \n",
    "It's easiest to do this using `keras.layers.TextVectorization` layer to encode this text.\n",
    "We set `split=\"character\"` to get character-level encoding rather than the default word-level encoding, and we use `standardize=\"lower\"` to convert the text to lowercase (which will simplify the task):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([21,  7, 10, ..., 22, 28, 12])>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\",\n",
    "                                                   standardize=\"lower\")\n",
    "text_vec_layer.adapt([shakespeare_text])\n",
    "encoded = text_vec_layer([shakespeare_text])[0]\n",
    "encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each character is now mapped to an integer, starting at 2. The `TextVectorization` layer reserved the value 0 for padding tokens, and it reserved 1 for unknown characters. We won’t need either of these tokens for now, so let’s subtract 2 from the character IDs and compute the number of distinct characters and the total number of characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  1115394\n",
      "Number of tokens:  39\n"
     ]
    }
   ],
   "source": [
    "encoded -= 2  # drop tokens 0 (pad) and 1 (unknown), which we will not use\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2  # number of distinct chars = 39\n",
    "dataset_size = len(encoded)  # total number of chars = 1,115,394\n",
    "\n",
    "print(\"Dataset size: \", dataset_size)\n",
    "print(\"Number of tokens: \", n_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a sequence-to-sequence RNN, we can convert this long sequence into input/target pairs. This dataset creation involves dividing the data into windows of a fixed size. For instance, a dataset sequence of the text \"to be or not to b\" will be turned into input as \"to be or not to\" sequence and target as \"o be or not to be\" sequence. This target sequence indicates that for a given input sequence, the next character should be \"e\". The model can then be trained on these input/target pairs to learn the underlying patterns in the text and generate more text of a similar style. \n",
    "\n",
    "The function `to_dataset` will convert our long sequence of character IDs (encoded text) into a dataset of input/target window pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
    "\n",
    "    # Prepare dataset of character IDs to be processed by tensorflow\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "\n",
    "    # Create windows of size length + 1\n",
    "    ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(100_000, seed=seed)\n",
    "\n",
    "    # Batch the resulting dataset\n",
    "    ds = ds.batch(batch_size)\n",
    "\n",
    "    # Create input/output sequences by taking first *length* characters as input\n",
    "    # and last *length* characters as output\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This diagram illustrates what `to_dataset` is doing:\n",
    "\n",
    "<img src=\"to_dataset.png\" width=\"500\" style=\"display: block; margin: 0 auto\">\n",
    "\n",
    "Batching is a technique used to divide large datasets into smaller subsets or batches.\n",
    "Instead of feeding the entire dataset (of our input/output pairs of windows) to the model at once, we divide it into batches, which are fed to the model one-by-one during training.\n",
    "Each batch is processed independently, and the model updates its weights after processing each batch.\n",
    "Batching makes training more efficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[ 4,  5,  2, 23]])>,\n",
       "  <tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[ 5,  2, 23,  3]])>)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example using to_dataset()\n",
    "# This code creates a dataset with a single training example: an input/output pair\n",
    "# The input represents \"to b\" and the output represents \"o be\", so the model \n",
    "# should learn to predict the next character, i.e., \"e\"\n",
    "list(to_dataset(text_vec_layer([\"To be\"])[0], length=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the entire dataset is 1,115,394 characters long and we have limited time, we will use a smaller portion of the dataset to make sure we can finish training during this workshop.\n",
    "We will split it up so we use roughly 90% for training, 5% for validation and the remaining 5% for testing.\n",
    "\n",
    "We initially specified the window length as 100, but it is worth experimenting with different window lengths.\n",
    "While shorter lengths make it easier and quicker to train the RNN, as the RNN is not able to learn any pattern that is longer than the specified length, it is important to avoid choosing a window length that is too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 100\n",
    "subset_proportion = 0.5\n",
    "reduced_dataset_size = int(dataset_size * subset_proportion)\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "train_set = to_dataset(encoded[:int(reduced_dataset_size * 0.9)], length=length, shuffle=True,\n",
    "                       seed=42)\n",
    "valid_set = to_dataset(encoded[int(reduced_dataset_size * 0.9):int(reduced_dataset_size * 0.95)], length=length)\n",
    "test_set = to_dataset(encoded[int(reduced_dataset_size * 0.95):reduced_dataset_size], length=length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Training the Char-RNN Model\n",
    "\n",
    "**Warning**: the following code may one or two hours to run, depending on your GPU. Without a GPU, it may take over 24 hours. If you don't want to wait, just skip the next two code cells and run the code below to download a pretrained model.\n",
    "\n",
    "To make GPU work, in terminal: `python -m pip install tensorflow-metal`\n",
    "\n",
    "**Note**: the `GRU` class will only use cuDNN acceleration (assuming you have a GPU) when using the default values for the following arguments: `activation`, `recurrent_activation`, `recurrent_dropout`, `unroll`, `use_bias` and `reset_after`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our dataset is reasonably large, and modeling language is quite a difficult task, we need more than a simple RNN with a few recurrent neurons.\n",
    "Let’s build and train a model with one GRU layer composed of 128 units (you can try tweaking the number of layers and units later, if needed).\n",
    "\n",
    "Let’s go over this code:\n",
    "\n",
    "- We use an `Embedding` layer as the first layer, to encode the character IDs (embeddings were introduced in Chapter 13). The `Embedding` layer’s number of input dimensions is the number of distinct character IDs, and the number of output dimensions is a hyperparameter you can tune—we’ll set it to 16 for now. Whereas the inputs of the `Embedding` layer will be 2D tensors of shape *[batch size, window length]*, the output of the Embedding layer will be a 3D tensor of shape *[batch size, window length, embedding size]*.\n",
    "\n",
    "- We use a `Dense` layer for the output layer: it must have 39 units (n_tokens) because there are 39 distinct characters in the text, and we want to output a probability for each possible character (at each time step). The 39 output probabilities should sum up to 1 at each time step, so we apply the softmax activation function to the outputs of the Dense layer.\n",
    "\n",
    "- Lastly, we compile this model, using the `\"sparse_categorical_crossentropy\"` loss and a Nadam optimizer, and we train the model for several epochs, using a `ModelCheckpoint` callback to save the best model (in terms of validation accuracy) as training progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: this takes too long, can we get GPU in Github Workspaces?\n",
    "\n",
    "# tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "#     tf.keras.layers.GRU(128, return_sequences=True),\n",
    "#     tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "# ])\n",
    "# model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "#               metrics=[\"accuracy\"])\n",
    "# model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     \"my_shakespeare_model\", monitor=\"val_accuracy\", save_best_only=True)\n",
    "# history = model.fit(train_set, validation_data=valid_set, epochs=10,\n",
    "#                     callbacks=[model_ckpt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model does not handle text preprocessing, so let’s wrap it in a final model containing the `tf.keras.layers.TextVectorization` layer as the first layer, plus a `tf.keras.layers.Lambda` layer to subtract 2 from the character IDs (since we’re not using the padding and unknown tokens for now):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shakespeare_model = tf.keras.Sequential([\n",
    "#     text_vec_layer,\n",
    "#     tf.keras.layers.Lambda(lambda X: X - 2),  # no <PAD> or <UNK> tokens\n",
    "#     model\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't want to wait for training to complete, we have a pretrained model for you.\n",
    "The following code will download it.\n",
    "Uncomment the last line if you want to use it instead of the model trained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5482547038441510248\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# extra code – downloads a pretrained model\n",
    "with tf.device('/CPU:0'):\n",
    "    url = \"https://github.com/ageron/data/raw/main/shakespeare_model.tgz\"\n",
    "    path = tf.keras.utils.get_file(\"shakespeare_model.tgz\", url, extract=True) \n",
    "    model_path = Path(path).with_name(\"shakespeare_model\")\n",
    "    shakespeare_model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.src.layers.preprocessing.index_lookup.VocabWeightHandler at 0x30fc39fc0>,\n",
       " <tf.Variable 'embedding/embeddings:0' shape=(39, 16) dtype=float32, numpy=\n",
       " array([[ 1.32050097e+00, -2.59510159e-01, -4.11443859e-02,\n",
       "         -3.42633694e-01, -1.31173706e+00,  1.34531796e-01,\n",
       "          6.91836357e-01,  7.36093968e-02, -7.89562762e-01,\n",
       "         -1.52055562e-01, -8.17673504e-01,  7.70714700e-01,\n",
       "         -1.44369400e+00, -5.65679260e-02,  1.52639374e-01,\n",
       "          5.92843831e-01],\n",
       "        [-7.78888166e-01,  6.68907881e-01, -6.08024187e-03,\n",
       "         -7.67722666e-01, -3.88999522e-01, -8.06803167e-01,\n",
       "          4.08726960e-01, -1.02228457e-02, -8.20410475e-02,\n",
       "         -1.16070993e-02,  1.23549990e-01,  2.96948791e-01,\n",
       "          1.32559168e+00,  2.00111166e-01,  1.09568262e+00,\n",
       "         -9.38163251e-02],\n",
       "        [ 1.02407813e+00, -9.66559649e-02,  1.31750107e-01,\n",
       "         -5.60444713e-01,  3.55749339e-01,  2.71324843e-01,\n",
       "         -9.60624337e-01, -6.04912732e-03, -2.26338282e-01,\n",
       "          1.26296210e+00,  1.46877170e+00, -7.04506159e-01,\n",
       "         -4.74435896e-01, -1.28502011e-01,  5.84523499e-01,\n",
       "         -4.06354159e-01],\n",
       "        [-3.31941545e-01,  7.34224319e-01,  1.00906909e+00,\n",
       "         -9.78566229e-01,  4.29281712e-01,  1.78255171e-01,\n",
       "         -1.01579010e+00, -6.03078663e-01,  5.93755126e-01,\n",
       "          2.49449447e-01, -5.81396043e-01, -9.74017859e-01,\n",
       "         -4.65091020e-01, -1.09726608e+00, -3.05734903e-01,\n",
       "          6.63186908e-01],\n",
       "        [-4.89908397e-01, -1.07185137e+00, -8.54912221e-01,\n",
       "         -5.13677374e-02, -2.51290649e-01,  6.13589287e-01,\n",
       "         -3.95226836e-01, -2.03877091e+00,  5.06859422e-01,\n",
       "         -4.40428704e-01, -2.65066445e-01,  6.09277785e-01,\n",
       "          4.28019345e-01, -5.46946228e-01,  4.92359459e-01,\n",
       "          4.96288419e-01],\n",
       "        [-1.68687201e+00, -4.22748238e-01,  3.89184535e-01,\n",
       "         -1.64510474e-01, -1.88834116e-01, -1.19694388e+00,\n",
       "         -7.32408166e-01,  8.70691001e-01,  5.71988106e-01,\n",
       "         -5.10193229e-01, -6.92926228e-01,  1.13621664e+00,\n",
       "         -5.30815661e-01, -7.44356036e-01, -7.49927342e-01,\n",
       "          5.71218312e-01],\n",
       "        [ 7.16338575e-01, -1.24078453e-01,  1.65790826e-01,\n",
       "          8.33971381e-01,  2.86951542e-01,  2.43417934e-01,\n",
       "          4.73711729e-01,  6.78981960e-01,  1.98199138e-01,\n",
       "         -7.27086306e-01,  6.67254210e-01, -5.34227848e-01,\n",
       "          1.03614891e+00,  9.71647948e-02,  1.01562515e-01,\n",
       "         -4.59411025e-01],\n",
       "        [-4.92716819e-01,  2.73131818e-01,  7.55887508e-01,\n",
       "          1.35912645e+00, -4.43237424e-02,  9.78558958e-01,\n",
       "         -1.09567142e+00, -3.64590734e-01, -1.65352643e+00,\n",
       "          3.56079221e-01, -5.07677570e-02, -4.61244404e-01,\n",
       "          2.67047644e-01,  4.05381881e-02, -8.30897912e-02,\n",
       "         -5.03838956e-01],\n",
       "        [-8.61649632e-01,  4.40835953e-01, -8.75075042e-01,\n",
       "          6.91622317e-01, -5.92501879e-01,  5.63054085e-01,\n",
       "          6.59617186e-01, -2.30482340e-01,  3.67211998e-01,\n",
       "          6.50802180e-02,  9.86428201e-01,  2.89367944e-01,\n",
       "          8.25816020e-02,  2.79242128e-01, -1.62656188e+00,\n",
       "          1.44519322e-02],\n",
       "        [-7.93637335e-01,  1.64046437e-01, -4.03555632e-01,\n",
       "          1.95794690e+00,  1.24737620e+00,  5.03970444e-01,\n",
       "          9.39080775e-01,  2.12669577e-02,  3.83359849e-01,\n",
       "          2.47803599e-01,  6.99558258e-02, -2.71937132e-01,\n",
       "         -4.53502446e-01,  4.52370793e-01,  1.20805550e+00,\n",
       "          8.29054594e-01],\n",
       "        [ 1.59992647e+00,  3.48127812e-01, -3.46482247e-01,\n",
       "          1.60694152e-01, -1.13532186e+00,  5.21970987e-01,\n",
       "          1.12796617e+00,  5.35396636e-01, -1.41718403e-01,\n",
       "          6.27965033e-01, -1.26796317e+00,  1.47618735e+00,\n",
       "         -1.95101726e+00, -1.29831898e+00,  5.77023685e-01,\n",
       "          6.43969998e-02],\n",
       "        [ 6.37181163e-01,  2.24164456e-01, -2.00990438e+00,\n",
       "          1.04017347e-01,  7.67671287e-01, -9.24697459e-01,\n",
       "         -5.26262932e-02,  3.16144079e-01,  2.55594492e-01,\n",
       "         -8.67595255e-01, -8.58588219e-01, -5.15984654e-01,\n",
       "          6.26067400e-01,  9.55120146e-01, -8.37178588e-01,\n",
       "          9.26504374e-01],\n",
       "        [-5.39789259e-01,  1.31938398e-01, -9.51873422e-01,\n",
       "         -1.19482076e+00,  7.25310683e-01,  1.68495548e+00,\n",
       "          2.39345178e-01,  1.91762105e-01,  4.17925119e-02,\n",
       "          9.62439895e-01,  3.87614876e-01, -1.25006158e-02,\n",
       "          2.18230754e-01,  9.29999292e-01, -9.81179699e-02,\n",
       "         -3.68350506e-01],\n",
       "        [-2.32795760e-01, -2.73007125e-01,  1.18700206e+00,\n",
       "         -4.30589706e-01, -7.43443131e-01,  9.13625479e-01,\n",
       "         -1.46915531e+00,  2.05431536e-01,  2.11962312e-01,\n",
       "         -1.71714914e+00,  5.39088130e-01,  1.25898468e+00,\n",
       "          4.92958933e-01, -1.64010286e-01,  4.51032281e-01,\n",
       "         -1.11847378e-01],\n",
       "        [-7.23393932e-02, -1.50842690e+00,  7.00862825e-01,\n",
       "         -1.52675912e-01, -5.01511246e-03, -1.30498692e-01,\n",
       "          3.18714708e-01,  7.37695098e-01,  7.17602253e-01,\n",
       "          1.07140529e+00, -4.00388241e-01, -2.13710845e-01,\n",
       "          3.62718046e-01,  8.79823089e-01, -1.01680338e+00,\n",
       "         -1.24404207e-01],\n",
       "        [-1.06888235e+00, -4.71899569e-01, -5.81297338e-01,\n",
       "          2.80149162e-01,  9.62472796e-01, -6.16082609e-01,\n",
       "         -9.87170160e-01,  2.95688719e-01,  8.10847342e-01,\n",
       "          5.24214685e-01, -4.05562222e-02, -1.06922224e-01,\n",
       "         -9.79152322e-01,  1.70843935e+00,  1.41877317e+00,\n",
       "         -1.04383576e+00],\n",
       "        [ 4.95171010e-01, -2.59711444e-01, -6.34745419e-01,\n",
       "         -2.00354815e-01, -4.95915800e-01, -2.55244672e-01,\n",
       "         -3.39352548e-01,  1.41902626e+00,  1.30717531e-01,\n",
       "         -8.63333404e-01,  1.96885490e+00, -2.79412568e-01,\n",
       "         -5.33696175e-01,  6.88057601e-01,  1.59401715e+00,\n",
       "         -1.26768744e+00],\n",
       "        [-3.15812714e-02, -1.19279468e+00, -1.57615197e+00,\n",
       "          4.46708411e-01, -4.87243757e-02,  3.87718707e-01,\n",
       "         -8.90412033e-01,  1.22576451e+00,  3.71505946e-01,\n",
       "          5.48829556e-01, -1.96726334e+00, -2.30799389e+00,\n",
       "          6.14970565e-01, -2.47661042e+00,  2.26329327e+00,\n",
       "         -2.76239657e+00],\n",
       "        [ 1.29277730e+00,  6.54166222e-01,  6.69633627e-01,\n",
       "          4.04724449e-01,  9.02754724e-01, -1.04482460e+00,\n",
       "          8.26130569e-01, -1.52712870e+00, -2.54292309e-01,\n",
       "         -5.83381474e-01,  7.49777019e-01, -5.95223546e-01,\n",
       "         -4.63714004e-01,  6.74386382e-01, -1.92874089e-01,\n",
       "         -6.61413670e-01],\n",
       "        [-3.65530014e-01, -1.08095729e+00, -4.94172156e-01,\n",
       "         -3.92580070e-02,  2.87052661e-01, -2.84980297e-01,\n",
       "          4.24756072e-02,  8.53987813e-01, -1.02584279e+00,\n",
       "          4.23124075e-01,  2.24691343e+00, -7.98678458e-01,\n",
       "         -5.18525839e-01, -1.69965148e+00,  3.09576780e-01,\n",
       "          1.63891244e+00],\n",
       "        [-8.96783173e-01,  1.15606701e+00, -8.10943782e-01,\n",
       "         -1.54323184e+00,  1.19870353e+00,  1.22283578e+00,\n",
       "          5.30158043e-01, -2.85234839e-01, -6.70542777e-01,\n",
       "         -1.31147254e+00,  5.13314605e-01, -6.31767094e-01,\n",
       "         -1.13252342e+00, -8.22509378e-02, -5.17517067e-02,\n",
       "         -1.62518847e+00],\n",
       "        [ 5.66625953e-01, -1.62994504e+00, -2.29467154e-01,\n",
       "         -9.59404826e-01,  1.20978785e+00,  8.18204165e-01,\n",
       "          8.78223836e-01,  8.07110608e-01,  5.65072238e-01,\n",
       "         -4.89232719e-01, -1.63595393e-01, -2.62618244e-01,\n",
       "          4.59505856e-01, -9.53247547e-01, -1.00358486e+00,\n",
       "          1.18563630e-01],\n",
       "        [ 1.00594747e+00,  4.89717089e-02, -5.71855664e-01,\n",
       "         -1.17860429e-01,  1.44470501e+00, -7.98541129e-01,\n",
       "         -4.36691463e-01,  6.86366856e-03, -6.65784061e-01,\n",
       "          6.09965146e-01,  1.93840191e-02,  1.41097677e+00,\n",
       "          3.86495948e-01, -7.18303919e-01, -7.98458755e-01,\n",
       "         -9.32802200e-01],\n",
       "        [-1.18934071e+00, -1.60385466e+00, -3.36696935e+00,\n",
       "          6.60812616e-01, -2.04022241e+00,  1.49157858e+00,\n",
       "         -4.51482087e-01,  2.59151077e+00, -9.40228581e-01,\n",
       "          6.45144463e-01, -2.25534225e+00, -2.16676140e+00,\n",
       "          1.36302912e+00, -4.24482155e+00,  1.35296285e+00,\n",
       "         -3.69162560e+00],\n",
       "        [-9.95570362e-01, -2.25865436e+00,  4.96569425e-01,\n",
       "         -4.27248806e-01,  8.12584400e-01, -3.36236089e-01,\n",
       "          8.60741735e-01,  3.85929823e-01, -2.12272120e+00,\n",
       "         -3.29337776e-01,  1.10733554e-01,  1.35538474e-01,\n",
       "          4.39708501e-01,  3.57720464e-01, -2.32705936e-01,\n",
       "         -1.48852682e+00],\n",
       "        [ 1.04753399e+00, -1.15214932e+00, -1.18663871e+00,\n",
       "          8.11405182e-01,  1.33653283e+00,  3.67655121e-02,\n",
       "          6.52490973e-01,  1.31869268e+00, -1.72719848e+00,\n",
       "          6.60872579e-01,  1.16427243e+00, -6.50428474e-01,\n",
       "         -8.48170578e-01,  4.24964279e-01, -1.85472643e+00,\n",
       "         -7.02623844e-01],\n",
       "        [ 3.01896948e-02, -1.52431786e+00, -7.62229300e+00,\n",
       "          2.11898255e+00, -4.42608213e+00,  1.30791655e-02,\n",
       "         -3.34270477e+00,  2.29190302e+00,  6.03129715e-02,\n",
       "          1.10583317e+00, -4.74836254e+00, -5.60046244e+00,\n",
       "         -8.85460556e-01, -4.87572479e+00,  1.61793396e-01,\n",
       "         -3.11490488e+00],\n",
       "        [-1.49908453e-01,  9.17471349e-01, -3.23954999e-01,\n",
       "          1.03880262e+00,  9.43328202e-01,  3.09435058e+00,\n",
       "         -8.44386756e-01,  1.27953756e+00, -7.44565800e-02,\n",
       "         -1.60164225e+00, -1.00117576e+00,  1.38031828e+00,\n",
       "          1.77307045e+00, -7.98438132e-01,  1.97285950e+00,\n",
       "          1.27819479e+00],\n",
       "        [ 1.94758289e-02, -1.70209968e+00, -2.22002387e+00,\n",
       "          1.25601304e+00, -2.17646956e+00,  7.20898569e-01,\n",
       "         -2.02407479e+00,  2.85098863e+00, -1.08767545e+00,\n",
       "         -8.81799590e-03, -1.67876232e+00, -4.41168976e+00,\n",
       "          2.14155412e+00, -5.18400431e+00,  2.13841128e+00,\n",
       "         -2.62049389e+00],\n",
       "        [-3.44413370e-01, -1.32918990e+00, -5.59179974e+00,\n",
       "          1.35530806e+00, -3.00267124e+00, -1.52235985e+00,\n",
       "         -1.98100805e+00,  7.01832592e-01,  8.58080089e-01,\n",
       "          2.45395851e+00, -4.97585344e+00, -4.73743725e+00,\n",
       "         -7.23472655e-01, -3.66983509e+00, -3.50104511e-01,\n",
       "         -5.15884733e+00],\n",
       "        [-3.73383373e-01,  1.01537064e-01, -4.51543617e+00,\n",
       "          6.94099545e-01, -3.06115365e+00, -2.17129096e-01,\n",
       "         -2.05805779e+00,  1.14349079e+00,  7.24330498e-03,\n",
       "          1.09755754e+00, -3.99368906e+00, -4.23910761e+00,\n",
       "          1.80550933e-01, -2.79259443e+00, -6.57535493e-01,\n",
       "         -3.63738775e+00],\n",
       "        [ 9.04625058e-01, -6.09395742e-01,  9.97731328e-01,\n",
       "          4.68232296e-02, -2.25628161e+00, -3.26272070e-01,\n",
       "          1.79989028e+00, -2.10349441e-01,  3.45058739e-01,\n",
       "          2.94214357e-02, -2.61019325e+00, -4.48770463e-01,\n",
       "         -1.04739308e+00, -7.29560852e-01,  3.54397625e-01,\n",
       "         -2.11524129e+00],\n",
       "        [-8.40975404e-01, -1.53188372e+00, -7.44280815e-01,\n",
       "         -2.72169292e-01,  3.47350895e-01,  4.43592593e-02,\n",
       "          1.65808475e+00, -7.04881489e-01, -1.69550717e+00,\n",
       "          3.62733245e-01,  5.78198731e-01, -1.50137138e+00,\n",
       "         -6.12096965e-01, -9.93532598e-01, -2.05260158e+00,\n",
       "          1.09925056e+00],\n",
       "        [ 7.69460142e-01,  5.06237328e-01,  6.05933107e-02,\n",
       "         -1.42158604e+00,  2.07761812e+00,  2.38033104e+00,\n",
       "         -4.07833904e-02,  2.83879697e-01, -1.11083508e+00,\n",
       "         -2.14031863e+00, -1.08446348e+00, -2.52477360e+00,\n",
       "         -2.11357498e+00, -8.87315392e-01, -1.96881592e+00,\n",
       "          3.68063673e-02],\n",
       "        [ 6.41873330e-02, -5.91113828e-02,  1.57004893e-01,\n",
       "          1.10769916e+00, -5.66311538e-01, -4.79858190e-01,\n",
       "         -3.09916377e-01, -2.06655692e-02, -4.23303060e-02,\n",
       "          1.56502795e+00,  1.01090693e+00,  5.29949009e-01,\n",
       "         -5.21381617e-01, -6.74940288e-01, -5.20440996e-01,\n",
       "         -9.51368809e-01],\n",
       "        [ 1.41752243e+00,  3.47401798e-01,  1.13004124e+00,\n",
       "          8.13864052e-01, -1.87426239e-01, -7.62082338e-01,\n",
       "          1.24049254e-01,  1.42140913e+00, -9.17887568e-01,\n",
       "          3.92772019e-01, -4.41861331e-01, -1.85190827e-01,\n",
       "          1.05705059e+00,  1.73360419e+00, -1.29169810e+00,\n",
       "         -9.37568545e-01],\n",
       "        [ 9.03178602e-02, -9.33961928e-01, -1.16438818e+00,\n",
       "         -1.04305720e+00,  8.15497816e-01,  2.08276606e+00,\n",
       "          1.44928670e+00,  2.25095391e+00,  2.10502601e+00,\n",
       "         -3.63272786e-01, -1.58605444e+00, -3.02389443e-01,\n",
       "         -7.12337136e-01, -2.55175614e+00,  1.63110709e+00,\n",
       "         -3.58093113e-01],\n",
       "        [-7.99416959e-01,  1.92674976e-02,  4.24514920e-01,\n",
       "         -4.89158258e-02, -3.69037464e-02, -9.04906750e-01,\n",
       "         -4.95186388e-01,  5.44435382e-02,  1.54921496e+00,\n",
       "          8.76602471e-01, -8.66147339e-01,  1.11409974e+00,\n",
       "         -3.21692884e-01,  2.87288725e-01, -4.57723647e-01,\n",
       "          2.13061236e-02],\n",
       "        [-2.18071938e-02, -3.42759639e-01, -5.03621757e-01,\n",
       "         -1.28343210e-01,  5.83224952e-01, -1.04494750e+00,\n",
       "         -4.78487551e-01,  6.83442578e-02,  5.88989079e-01,\n",
       "         -6.43113673e-01,  1.24658950e-01, -6.27918482e-01,\n",
       "          4.03038740e-01, -1.40752971e-01,  5.78369498e-01,\n",
       "          4.24545825e-01]], dtype=float32)>,\n",
       " <tf.Variable 'gru/gru_cell/kernel:0' shape=(16, 384) dtype=float32, numpy=\n",
       " array([[-0.8698067 ,  0.4984082 , -0.36424735, ...,  1.5142806 ,\n",
       "          0.2560526 ,  0.965835  ],\n",
       "        [ 0.84867775,  0.14782338,  0.44029847, ...,  0.48590723,\n",
       "          0.40347585, -0.46246687],\n",
       "        [ 2.3350089 , -0.08491813,  1.2791806 , ..., -0.43235862,\n",
       "         -0.09110921,  0.81107974],\n",
       "        ...,\n",
       "        [-1.1317071 ,  0.39646038,  2.4403136 , ..., -1.8690901 ,\n",
       "         -0.6250112 , -1.0296584 ],\n",
       "        [ 0.44347268,  0.04983211,  2.2117317 , ...,  0.552357  ,\n",
       "          0.56688726,  1.8390291 ],\n",
       "        [ 0.65457386,  0.34310612, -0.38716954, ..., -0.7433537 ,\n",
       "         -0.05211578, -2.2542353 ]], dtype=float32)>,\n",
       " <tf.Variable 'gru/gru_cell/recurrent_kernel:0' shape=(128, 384) dtype=float32, numpy=\n",
       " array([[ 0.39661303, -0.08966733, -0.39559582, ...,  0.28441715,\n",
       "         -0.01547177, -0.14504188],\n",
       "        [ 0.5147598 , -0.25025073,  0.76061225, ..., -0.33289874,\n",
       "          0.15209745, -0.34358576],\n",
       "        [-1.4789114 ,  0.59020495, -1.5005723 , ...,  0.40949678,\n",
       "         -0.36563155, -0.09943692],\n",
       "        ...,\n",
       "        [-1.0437632 ,  0.1585398 , -2.2751696 , ..., -0.43923208,\n",
       "          0.09748349, -0.29104546],\n",
       "        [ 0.14405261,  0.02816918,  0.16256422, ...,  0.33111793,\n",
       "          0.84065235, -0.2166497 ],\n",
       "        [ 0.4526033 , -0.38022   , -0.34015754, ..., -0.09450489,\n",
       "         -0.19275156, -0.25756747]], dtype=float32)>,\n",
       " <tf.Variable 'gru/gru_cell/bias:0' shape=(2, 384) dtype=float32, numpy=\n",
       " array([[-0.13548423, -0.7550572 ,  0.4451452 , -0.79078174, -1.0267187 ,\n",
       "          0.31549424, -1.3632474 , -0.39148983,  0.20833963,  0.91488117,\n",
       "         -0.6860293 , -1.6248405 , -0.35477006, -0.49507323, -0.45766369,\n",
       "         -0.10749672, -1.4903584 , -0.45964164,  0.73639005, -0.7270562 ,\n",
       "          0.3839181 , -4.095017  , -0.12625098, -1.3905824 ,  0.3969079 ,\n",
       "         -2.0350182 , -1.0497974 ,  0.18816341, -1.2894542 ,  0.71171385,\n",
       "         -2.8978658 , -1.682478  , -1.9021738 , -0.8419898 , -1.7415416 ,\n",
       "         -0.56146306, -0.2832652 ,  0.17304689, -0.11755513, -1.7589971 ,\n",
       "         -2.5736666 , -3.5351093 , -0.3547595 , -1.2633686 , -0.9255421 ,\n",
       "         -0.89481723, -1.9436964 , -2.268216  , -0.2747439 ,  0.67674583,\n",
       "         -1.5815595 , -2.0152876 , -1.1932409 , -1.3372381 , -0.39992204,\n",
       "         -0.3428742 , -0.78407174, -1.6279383 , -1.4731395 , -3.3270786 ,\n",
       "          0.04210451, -2.4237857 , -1.4629768 , -0.2710001 , -0.11257432,\n",
       "         -1.009486  , -2.1860046 ,  0.09919476, -1.0520889 , -1.1495246 ,\n",
       "         -0.02355812, -0.976234  , -0.62184954, -1.8792663 , -1.0838892 ,\n",
       "         -0.6704725 , -1.4788893 , -2.1484823 , -1.6083826 , -0.6116329 ,\n",
       "          0.1444796 , -0.50931716,  0.12024482, -0.23796234, -1.6959441 ,\n",
       "          0.09961671,  0.66080636, -1.5254433 , -0.30914733, -2.3022869 ,\n",
       "         -1.9768384 , -0.10898472,  0.43623963, -0.65544206, -0.10136016,\n",
       "          0.27025467, -0.5068123 , -1.1817088 , -0.6659347 , -0.9618664 ,\n",
       "         -1.9265478 , -0.19911334,  0.4050083 ,  0.04797636,  0.50065714,\n",
       "          0.9734014 ,  0.81182283, -0.59418803, -1.6215514 , -1.6452649 ,\n",
       "         -0.2900789 ,  0.04851441,  0.07204807, -0.92319405, -0.09971477,\n",
       "         -0.29844287,  0.02210275, -0.62643903, -2.8028672 , -1.3199079 ,\n",
       "         -0.07166784,  0.16498297,  0.3288579 , -0.30057934, -0.47182706,\n",
       "         -0.14564109,  0.50056976, -0.3828236 , -0.27284217,  0.24823537,\n",
       "          0.09643929, -0.8869814 ,  1.4012439 , -0.8273005 , -0.148836  ,\n",
       "         -0.18996574, -0.79163176,  1.3697821 ,  0.443417  ,  0.22223695,\n",
       "          0.58507675, -0.79908204, -0.36327583, -0.10662801,  0.31652215,\n",
       "         -0.18430652, -0.36212578, -0.7126883 , -0.37161002,  0.97350496,\n",
       "         -1.1274874 , -1.1359001 ,  0.7067928 , -0.9373824 ,  0.2991431 ,\n",
       "         -0.54013103,  0.8099803 , -0.31481284, -0.6134054 , -0.7539017 ,\n",
       "         -1.2279253 ,  0.7155224 ,  1.0000064 , -0.04403013,  0.36690772,\n",
       "         -0.1417022 ,  0.19073422,  0.64755285,  1.386961  , -1.736448  ,\n",
       "          1.222134  ,  0.39650923, -0.31547147, -0.19738112,  2.1687002 ,\n",
       "         -0.14822674,  0.20218715,  2.9170642 , -0.97664666, -0.06641603,\n",
       "         -0.34238404,  1.5630788 , -0.77263635, -0.3681151 ,  1.1791661 ,\n",
       "          0.73447174,  0.9459578 ,  0.08123609, -0.13729851,  0.71621966,\n",
       "         -0.05234766,  0.09377345, -0.07919872, -0.62419295,  2.4086697 ,\n",
       "         -0.25504264,  0.8060737 , -0.50619787, -0.41472661,  0.0572205 ,\n",
       "         -0.36632738, -1.0161115 ,  0.23082201, -0.9978296 ,  0.21388306,\n",
       "         -0.2679442 , -0.2203517 ,  0.8814109 ,  1.4607588 ,  0.18880211,\n",
       "          0.22905272, -1.1928416 ,  0.41530344, -0.88657945,  0.41124797,\n",
       "         -0.28502694, -0.72506946, -1.4381325 ,  0.11668152, -0.95755893,\n",
       "          0.34206468,  1.016164  , -1.9605279 , -0.45206946,  0.01027577,\n",
       "         -1.4893397 ,  0.8063404 ,  0.5503816 ,  1.9961053 ,  0.5512473 ,\n",
       "         -0.15925734, -0.38212135,  0.85930055,  0.39712086, -0.578372  ,\n",
       "         -0.14535016,  0.16717796, -0.44621247,  0.19943283,  1.3754122 ,\n",
       "          0.33738524,  0.60642314,  1.189384  ,  1.0377196 ,  0.16308755,\n",
       "         -0.03447421,  0.2813009 ,  0.45449048, -0.7725284 , -0.6213975 ,\n",
       "          0.25955454,  0.12161073,  1.4632131 ,  1.0764511 , -0.27764118,\n",
       "         -1.692086  , -0.6556416 , -0.17635228,  0.6370719 ,  0.01221054,\n",
       "         -0.27657232, -0.41223344,  0.12097159,  0.54978836,  0.21176623,\n",
       "          0.8691109 ,  0.60492736, -0.32720488, -0.10512107, -0.34847048,\n",
       "          0.06819928, -0.52165884, -0.36257774, -0.5445286 , -0.5166379 ,\n",
       "          0.83008754,  0.69699883,  0.97793466, -1.164807  , -0.29842368,\n",
       "          0.09030764, -0.2981302 ,  0.6992342 , -0.5110164 ,  0.7658316 ,\n",
       "          0.07862562, -0.10721944,  0.1019319 , -0.94782346,  0.01157691,\n",
       "         -0.9364791 ,  0.40514815,  1.0858655 , -1.4892381 ,  0.79200757,\n",
       "         -0.14341857,  0.6490664 , -0.77711785, -0.71057194, -0.78835696,\n",
       "          0.11424138,  0.44550365, -0.3635292 , -0.5696794 ,  0.42689848,\n",
       "          0.20813546,  0.04118213, -0.4652852 ,  0.5996737 ,  0.42669657,\n",
       "         -0.16811457,  0.2944959 ,  0.37525335, -0.70257276, -0.29206645,\n",
       "          0.84748834,  0.25432092, -0.6698627 , -0.3039307 , -0.07303206,\n",
       "         -0.19902515, -0.3882558 , -0.12366318, -0.5020664 , -0.3218145 ,\n",
       "          0.22794953,  0.5465959 ,  0.09087513, -1.3418314 ,  0.12112553,\n",
       "          0.29815122, -0.12240143, -0.47834417,  0.9233413 , -0.747004  ,\n",
       "          0.54545075,  0.4429654 , -0.2549282 ,  0.563713  ,  0.83861434,\n",
       "         -0.6601442 ,  0.6605976 , -0.7820537 , -0.40376914, -0.5910075 ,\n",
       "         -0.33210036, -0.10635198,  0.8638707 , -0.47017425,  0.4267958 ,\n",
       "         -0.11850461,  0.29593268, -0.2844064 , -0.03626415,  0.02750491,\n",
       "         -0.22698393,  0.45823434, -1.1845275 , -0.8141153 , -0.2535862 ,\n",
       "         -0.6392088 ,  0.3069222 ,  0.18320197, -0.6100269 , -0.01861994,\n",
       "          0.810603  ,  0.2534011 ,  0.36302155, -1.316342  , -0.6632347 ,\n",
       "          2.0536184 ,  0.09855673, -0.36520508,  0.07859619, -0.78208226,\n",
       "          0.8610294 , -0.11910551, -0.6410995 ,  0.6681736 , -0.16661735,\n",
       "         -0.4983268 ,  0.45829824, -0.60216796,  1.1340235 ],\n",
       "        [-0.13548423, -0.7550572 ,  0.4451452 , -0.79078174, -1.0267187 ,\n",
       "          0.31549424, -1.3632474 , -0.39148983,  0.20833963,  0.91488117,\n",
       "         -0.6860293 , -1.6248405 , -0.35477006, -0.49507323, -0.45766369,\n",
       "         -0.10749672, -1.4903584 , -0.45964164,  0.73639005, -0.7270562 ,\n",
       "          0.3839181 , -4.095017  , -0.12625098, -1.3905824 ,  0.3969079 ,\n",
       "         -2.0350182 , -1.0497974 ,  0.18816341, -1.2894542 ,  0.71171385,\n",
       "         -2.8978658 , -1.682478  , -1.9021738 , -0.8419898 , -1.7415416 ,\n",
       "         -0.56146306, -0.2832652 ,  0.17304689, -0.11755513, -1.7589971 ,\n",
       "         -2.5736666 , -3.5351093 , -0.3547595 , -1.2633686 , -0.9255421 ,\n",
       "         -0.89481723, -1.9436964 , -2.268216  , -0.2747439 ,  0.67674583,\n",
       "         -1.5815595 , -2.0152876 , -1.1932409 , -1.3372381 , -0.39992204,\n",
       "         -0.3428742 , -0.78407174, -1.6279383 , -1.4731395 , -3.3270786 ,\n",
       "          0.04210451, -2.4237857 , -1.4629768 , -0.2710001 , -0.11257432,\n",
       "         -1.009486  , -2.1860046 ,  0.09919476, -1.0520889 , -1.1495246 ,\n",
       "         -0.02355812, -0.976234  , -0.62184954, -1.8792663 , -1.0838892 ,\n",
       "         -0.6704725 , -1.4788893 , -2.1484823 , -1.6083826 , -0.6116329 ,\n",
       "          0.1444796 , -0.50931716,  0.12024482, -0.23796234, -1.6959441 ,\n",
       "          0.09961671,  0.66080636, -1.5254433 , -0.30914733, -2.3022869 ,\n",
       "         -1.9768384 , -0.10898472,  0.43623963, -0.65544206, -0.10136016,\n",
       "          0.27025467, -0.5068123 , -1.1817088 , -0.6659347 , -0.9618664 ,\n",
       "         -1.9265478 , -0.19911334,  0.4050083 ,  0.04797636,  0.50065714,\n",
       "          0.9734014 ,  0.81182283, -0.59418803, -1.6215514 , -1.6452649 ,\n",
       "         -0.2900789 ,  0.04851441,  0.07204807, -0.92319405, -0.09971477,\n",
       "         -0.29844287,  0.02210275, -0.62643903, -2.8028672 , -1.3199079 ,\n",
       "         -0.07166784,  0.16498297,  0.3288579 , -0.30057934, -0.47182706,\n",
       "         -0.14564109,  0.50056976, -0.3828236 , -0.27284217,  0.24823537,\n",
       "          0.09643929, -0.8869814 ,  1.4012439 , -0.8273005 , -0.148836  ,\n",
       "         -0.18996574, -0.79163176,  1.3697821 ,  0.443417  ,  0.22223695,\n",
       "          0.58507675, -0.79908204, -0.36327583, -0.10662801,  0.31652215,\n",
       "         -0.18430652, -0.36212578, -0.7126883 , -0.37161002,  0.97350496,\n",
       "         -1.1274874 , -1.1359001 ,  0.7067928 , -0.9373824 ,  0.2991431 ,\n",
       "         -0.54013103,  0.8099803 , -0.31481284, -0.6134054 , -0.7539017 ,\n",
       "         -1.2279253 ,  0.7155224 ,  1.0000064 , -0.04403013,  0.36690772,\n",
       "         -0.1417022 ,  0.19073422,  0.64755285,  1.386961  , -1.736448  ,\n",
       "          1.222134  ,  0.39650923, -0.31547147, -0.19738112,  2.1687002 ,\n",
       "         -0.14822674,  0.20218715,  2.9170642 , -0.97664666, -0.06641603,\n",
       "         -0.34238404,  1.5630788 , -0.77263635, -0.3681151 ,  1.1791661 ,\n",
       "          0.73447174,  0.9459578 ,  0.08123609, -0.13729851,  0.71621966,\n",
       "         -0.05234766,  0.09377345, -0.07919872, -0.62419295,  2.4086697 ,\n",
       "         -0.25504264,  0.8060737 , -0.50619787, -0.41472661,  0.0572205 ,\n",
       "         -0.36632738, -1.0161115 ,  0.23082201, -0.9978296 ,  0.21388306,\n",
       "         -0.2679442 , -0.2203517 ,  0.8814109 ,  1.4607588 ,  0.18880211,\n",
       "          0.22905272, -1.1928416 ,  0.41530344, -0.88657945,  0.41124797,\n",
       "         -0.28502694, -0.72506946, -1.4381325 ,  0.11668152, -0.95755893,\n",
       "          0.34206468,  1.016164  , -1.9605279 , -0.45206946,  0.01027577,\n",
       "         -1.4893397 ,  0.8063404 ,  0.5503816 ,  1.9961053 ,  0.5512473 ,\n",
       "         -0.15925734, -0.38212135,  0.85930055,  0.39712086, -0.578372  ,\n",
       "         -0.14535016,  0.16717796, -0.44621247,  0.19943283,  1.3754122 ,\n",
       "          0.33738524,  0.60642314,  1.189384  ,  1.0377196 ,  0.16308755,\n",
       "         -0.03447421,  0.2813009 ,  0.45449048, -0.7725284 , -0.6213975 ,\n",
       "          0.25955454,  0.12161073,  1.4632131 ,  1.0764511 , -0.27764118,\n",
       "         -1.692086  ,  0.02654073,  0.19168033,  0.27253732,  1.4575434 ,\n",
       "          0.65440375, -0.33106244, -0.2203696 ,  0.14359295,  0.64261216,\n",
       "         -1.4919821 , -0.09164442, -0.33711725, -0.8063546 ,  0.22645451,\n",
       "         -0.29664138,  0.4668317 , -0.7458161 , -0.2835633 , -0.21846129,\n",
       "          0.9768163 ,  0.13015586,  2.1033652 ,  0.03912551,  0.08709218,\n",
       "          0.7215    ,  0.02924053, -1.4262733 , -0.04407456, -0.07070034,\n",
       "         -1.3166245 ,  0.5071135 ,  0.17653008, -0.8279965 , -0.23075461,\n",
       "          1.1611922 ,  0.4905823 , -0.4384128 ,  0.12716821, -0.7753537 ,\n",
       "          1.3821687 , -0.4728918 , -0.15614718, -2.075504  , -1.1803775 ,\n",
       "          0.5201503 ,  0.81433696, -0.95157593, -0.11508873, -0.06974701,\n",
       "         -1.2829145 , -0.36310107, -1.288763  , -0.18272273,  0.60167557,\n",
       "          0.23149474,  0.14697307,  0.5106724 , -1.1287901 , -0.89922637,\n",
       "         -0.27967426, -0.1977374 , -0.6149273 ,  0.08026726,  2.5618472 ,\n",
       "         -0.3132858 ,  0.20975447,  0.4380282 , -0.2860514 , -0.42808992,\n",
       "          0.08913426,  0.62037224,  0.07799667, -0.2348097 ,  0.46185437,\n",
       "         -1.41676   ,  0.02610719, -0.5773956 ,  1.8245766 , -1.1498176 ,\n",
       "         -1.7302494 , -0.10053681, -0.42213508, -1.2551749 , -0.05932202,\n",
       "         -0.7245748 ,  1.6045411 , -0.12508048, -0.20371814, -0.40857854,\n",
       "         -0.33751553,  0.01883551,  0.79971427, -1.5184095 ,  0.26092264,\n",
       "         -0.8675442 , -0.9855415 ,  0.72010136, -0.87187177,  0.14447457,\n",
       "         -0.48032382,  0.6798597 ,  1.1813608 , -1.0939219 , -0.42487094,\n",
       "          1.2106968 ,  0.8090776 , -0.46660557,  0.43948677, -0.7899939 ,\n",
       "         -0.21708795,  0.394234  , -1.4519881 , -0.6988126 ,  0.53645444,\n",
       "          0.51502806, -0.04862865,  0.74607986, -0.16696951, -0.00896768,\n",
       "          0.11614639,  0.5522216 , -1.6496838 ,  0.5556935 , -1.3005773 ,\n",
       "         -0.94318503,  0.6913643 , -0.973203  ,  1.7564026 ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense/kernel:0' shape=(128, 39) dtype=float32, numpy=\n",
       " array([[ 0.71183705, -0.6194951 , -1.0980752 , ...,  3.13112   ,\n",
       "         -0.34036124, -0.12503113],\n",
       "        [ 0.04048581, -0.26425678, -0.13242805, ...,  1.0022616 ,\n",
       "          1.0249038 , -1.09071   ],\n",
       "        [ 0.00522425,  1.0727556 ,  0.9542762 , ..., -0.01262684,\n",
       "          0.1921757 ,  1.7290095 ],\n",
       "        ...,\n",
       "        [ 0.3839489 , -0.6750568 ,  0.2925147 , ..., -0.76900494,\n",
       "         -0.73378766, -0.6606745 ],\n",
       "        [-0.20246364, -0.5378494 , -0.8102419 , ...,  1.3400929 ,\n",
       "          2.187485  ,  1.1386892 ],\n",
       "        [ 0.27924892, -0.13143365,  0.22491652, ..., -0.6485381 ,\n",
       "         -1.9293034 , -0.4854912 ]], dtype=float32)>,\n",
       " <tf.Variable 'dense/bias:0' shape=(39,) dtype=float32, numpy=\n",
       " array([ 1.8102423 ,  0.29909787, -0.06425569, -0.47021806, -0.34073707,\n",
       "        -0.32008836,  1.2058344 ,  1.3612505 ,  0.46213436, -0.08470091,\n",
       "         0.35403106, -1.5931398 , -0.9429133 , -0.547732  ,  0.4419744 ,\n",
       "        -0.33157656, -1.7501014 ,  1.1289561 , -0.02454374,  1.789098  ,\n",
       "        -1.6051687 ,  0.93513864,  1.2181343 ,  1.0861257 ,  2.2607048 ,\n",
       "        -1.7936976 ,  0.19140232, -0.9802845 , -0.48946175, -0.532355  ,\n",
       "        -0.9977375 , -1.2065163 , -3.133314  , -4.692869  , -5.370412  ,\n",
       "        -4.0947084 , -1.5771215 , -3.5157936 , -2.605039  ], dtype=float32)>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakespeare_model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use it to predict the next character in the sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 195ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba = shakespeare_model.predict([\"To be or not to b\"])[0, -1]\n",
    "y_pred = tf.argmax(y_proba)  # choose the most probable character ID\n",
    "text_vec_layer.get_vocabulary()[y_pred + 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Fake Shakespearean Text\n",
    "\n",
    "Wee issue here: we cannot set tensorflow to use GPU for training and using the model. The output is poo-poo. CPU output is good. DILEMMA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate new text using the char-RNN model, we could feed it some text, make the model predict the most likely next letter, add it to the end of the text, then give the extended text to the model to guess the next letter, and so on. This is called greedy decoding. But in practice this often leads to the same words being repeated over and over again. Instead, we can sample the next character randomly, with a probability equal to the estimated probability, using TensorFlow’s tf.random.categorical() function. This will generate more diverse and interesting text. The categorical() function samples random class indices, given the class log probabilities (logits). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 8), dtype=int64, numpy=array([[0, 1, 0, 2, 1, 0, 0, 1]])>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probas = tf.math.log([[0.5, 0.4, 0.1]])  # probas = 50%, 40%, and 10%\n",
    "tf.random.categorical(log_probas, num_samples=8)  # draw 8 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have more control over the diversity of the generated text, we can divide the logits by a number called the temperature, which we can tweak as we wish. A temperature close to zero favors high-probability characters, while a high temperature gives all characters an equal probability. Lower temperatures are typically preferred when generating fairly rigid and precise text, such as mathematical equations, while higher temperatures are preferred when generating more diverse and creative text. The following next_char() custom helper function uses this approach to pick the next character to add to the input text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "\n",
    "    # Generate the predicted probabilities for the next character in the sequence\n",
    "    # based on the current text\n",
    "    # Select the final output vector from this prediction, i.e. the last character in the sequence\n",
    "    y_proba = shakespeare_model.predict([text])[0, -1:]\n",
    "    \n",
    "    # Rescale the probability distribution using the temperature parameter\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "\n",
    "    # Sample the next character ID from this rescaled distribution\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
    "\n",
    "    # Return the character corresponding to the sampled ID\n",
    "    return text_vec_layer.get_vocabulary()[char_id + 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can write another small helper function that will repeatedly call `next_char()` to get the next character and append it to the given text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "To be or not to be the duke\n",
      "as it is a proper strange death,\n",
      "and the\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "print(extend_text(\"To be or not to be\", temperature=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "To be or not to behold?\n",
      "\n",
      "second push:\n",
      "gremio, lord all, a sistermen,\n"
     ]
    }
   ],
   "source": [
    "print(extend_text(\"To be or not to be\", temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "To be or not to be edward knows\n",
      "whose stew, am i bid them for you, g\n"
     ]
    }
   ],
   "source": [
    "print(extend_text(\"To be or not to be\", temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "To be or not to bed tire\n",
      "the strangeness pity. what is't your allay,\n"
     ]
    }
   ],
   "source": [
    "print(extend_text(\"To be or not to be\", temperature=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edc2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
